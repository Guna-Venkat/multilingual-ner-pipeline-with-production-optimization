{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 0: Install dependencies\n!pip install transformers datasets evaluate seqeval accelerate wandb\n!pip install pandas numpy matplotlib seaborn\n!pip install sentencepiece sacremoses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T13:59:29.586476Z","iopub.execute_input":"2026-01-07T13:59:29.587186Z","iopub.status.idle":"2026-01-07T13:59:43.562517Z","shell.execute_reply.started":"2026-01-07T13:59:29.587156Z","shell.execute_reply":"2026-01-07T13:59:43.561807Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.1)\nCollecting evaluate\n  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\nCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\nRequirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from seqeval) (1.6.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\nRequirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.1)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.5)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.42.1)\nRequirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.11.12)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.1rc0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.6.2)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.3)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.3)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\nDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=18d1e09c5e1d341c693537be5bba1bd7be7401f679f9460f16be23165cddd120\n  Stored in directory: /root/.cache/pip/wheels/5f/b8/73/0b2c1a76b701a677653dd79ece07cfabd7457989dbfbdcd8d7\nSuccessfully built seqeval\nInstalling collected packages: seqeval, evaluate\nSuccessfully installed evaluate-0.4.6 seqeval-1.2.2\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\nCollecting sacremoses\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacremoses) (2025.11.3)\nRequirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from sacremoses) (8.3.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from sacremoses) (1.5.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sacremoses) (4.67.1)\nDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sacremoses\nSuccessfully installed sacremoses-0.1.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 1: Setup and imports\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom datasets import load_dataset, DatasetDict, concatenate_datasets\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForTokenClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForTokenClassification\n)\nfrom evaluate import load\nimport wandb\nimport os\nfrom sklearn.metrics import classification_report\nimport json\n\n# Set random seeds for reproducibility\ndef set_seed(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nset_seed(42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T13:59:43.563932Z","iopub.execute_input":"2026-01-07T13:59:43.564198Z","iopub.status.idle":"2026-01-07T14:00:14.072348Z","shell.execute_reply.started":"2026-01-07T13:59:43.564171Z","shell.execute_reply":"2026-01-07T14:00:14.071530Z"}},"outputs":[{"name":"stderr","text":"2026-01-07 13:59:57.253644: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767794397.409743      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767794397.453754      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767794397.813220      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767794397.813253      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767794397.813256      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767794397.813259      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 2: Configuration\nclass TeacherConfig:\n    MODEL_NAME = \"xlm-roberta-large\"\n    DATASET_NAME = \"wikiann\"\n    LANGUAGES = [\"en\", \"de\", \"fr\"]  # Start with 3 languages\n    MAX_LENGTH = 128\n    BATCH_SIZE = 16\n    GRADIENT_ACCUMULATION_STEPS = 2\n    LEARNING_RATE = 2e-5\n    NUM_EPOCHS = 3\n    WARMUP_RATIO = 0.1\n    WEIGHT_DECAY = 0.01\n    LABEL_NAMES = [\n        \"O\",\n        \"B-PER\", \"I-PER\",\n        \"B-ORG\", \"I-ORG\",\n        \"B-LOC\", \"I-LOC\"\n    ]\n    NUM_LABELS = 7\n    OUTPUT_DIR = \"./models/teacher\"\n    LOGGING_DIR = \"./logs/teacher\"\n\nconfig = TeacherConfig()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:00:14.073805Z","iopub.execute_input":"2026-01-07T14:00:14.074205Z","iopub.status.idle":"2026-01-07T14:00:14.078452Z","shell.execute_reply.started":"2026-01-07T14:00:14.074178Z","shell.execute_reply":"2026-01-07T14:00:14.077822Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Cell 3: Initialize W&B for experiment tracking\nwandb.init(\n    project=\"multilingual-ner\",\n    name=\"teacher-model-training\",\n    mode=\"offline\",  # <--- Add this line to go Offline\n    config={\n        \"model\": config.MODEL_NAME,\n        \"languages\": config.LANGUAGES,\n        \"batch_size\": config.BATCH_SIZE,\n        \"learning_rate\": config.LEARNING_RATE,\n        \"epochs\": config.NUM_EPOCHS\n    }\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:00:14.079233Z","iopub.execute_input":"2026-01-07T14:00:14.079513Z","iopub.status.idle":"2026-01-07T14:00:27.588631Z","shell.execute_reply.started":"2026-01-07T14:00:14.079483Z","shell.execute_reply":"2026-01-07T14:00:27.588126Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n  | |_| | '_ \\/ _` / _` |  _/ -_)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.22.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/kaggle/working/wandb/offline-run-20260107_140021-u7cbf6bb</code>"},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7a46a65e54c0>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Cell 4: Load and prepare multilingual dataset\n\ndef load_multilingual_dataset(languages, max_train_samples=20000):\n    datasets_dict = {}\n    \n    # 1. Load individual datasets from Hugging Face\n    for lang in languages:\n        try:\n            print(f\"Loading {lang} dataset...\")\n            datasets_dict[lang] = load_dataset(config.DATASET_NAME, lang)\n        except Exception as e:\n            print(f\"Error loading {lang}: {e}\")\n    \n    train_list, val_list, test_list = [], [], []\n    \n    # 2. Calculate how many samples to take per language to be fair\n    # This ensures an even distribution across your chosen languages\n    samples_per_lang = max_train_samples // len(languages)\n    eval_samples_per_lang = samples_per_lang // 4  # Keep validation smaller for speed\n\n    for lang, ds in datasets_dict.items():\n        # Shuffle and select a subset for each language to keep it representative\n        train_sub = ds['train'].shuffle(seed=42).select(range(min(samples_per_lang, len(ds['train']))))\n        val_sub = ds['validation'].shuffle(seed=42).select(range(min(eval_samples_per_lang, len(ds['validation']))))\n        \n        # Select a small portion of the test set for the specific language\n        test_sub = ds['test'].shuffle(seed=42).select(range(min(500, len(ds['test']))))\n        \n        # Add 'language' column to ALL splits to avoid ValueError during tokenization (.map)\n        train_list.append(train_sub.add_column(\"language\", [lang] * len(train_sub)))\n        val_list.append(val_sub.add_column(\"language\", [lang] * len(val_sub)))\n        test_list.append(test_sub.add_column(\"language\", [lang] * len(test_sub)))\n        \n    # 3. Combine everything into a single DatasetDict\n    # Using concatenate_datasets keeps the object as a Hugging Face Dataset (not a list)\n    combined_dataset = DatasetDict({\n        'train': concatenate_datasets(train_list).shuffle(seed=42),\n        'validation': concatenate_datasets(val_list).shuffle(seed=42),\n        'test': concatenate_datasets(test_list).shuffle(seed=42)\n    })\n    \n    # Print statistics for verification\n    print(f\"\\nCombined dataset sizes:\")\n    print(f\"  Train: {len(combined_dataset['train'])}\")\n    print(f\"  Validation: {len(combined_dataset['validation'])}\")\n    print(f\"  Test: {len(combined_dataset['test'])}\")\n    \n    return combined_dataset\n\n# Run the loader\ndataset = load_multilingual_dataset(config.LANGUAGES)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:00:35.904657Z","iopub.execute_input":"2026-01-07T14:00:35.905399Z","iopub.status.idle":"2026-01-07T14:00:55.039800Z","shell.execute_reply.started":"2026-01-07T14:00:35.905368Z","shell.execute_reply":"2026-01-07T14:00:55.039116Z"}},"outputs":[{"name":"stdout","text":"Loading en dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcbe615331734ab39b3810a008a8b740"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"en/validation-00000-of-00001.parquet:   0%|          | 0.00/748k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e96fa50a87b436da9ae95248055facf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"en/test-00000-of-00001.parquet:   0%|          | 0.00/748k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff91b9eaea9540f0b31622296b085e2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"en/train-00000-of-00001.parquet:   0%|          | 0.00/1.50M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1255d0c3fe7b4056aae318dbd2836e6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12f6555d86a944538fa9ece2ede8862e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03c6280a510d460e92e6af0187be33e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91aaf2adf8554603a4558dcd3aadf8bf"}},"metadata":{}},{"name":"stdout","text":"Loading de dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"de/validation-00000-of-00001.parquet:   0%|          | 0.00/835k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0175068a4f2042ce866fca3e7340dcae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"de/test-00000-of-00001.parquet:   0%|          | 0.00/832k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"003d204f08c949f3bac74fbcb9536da9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"de/train-00000-of-00001.parquet:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07ecf6cbd6424ff4b725c951924f38b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bafe406cf0584d33bd72df6bf6b0c674"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"785ed5e093df4f2f86d07f18e67a9fab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b91a3e6f1d7546aea6d54be8172a6f5b"}},"metadata":{}},{"name":"stdout","text":"Loading fr dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"fr/validation-00000-of-00001.parquet:   0%|          | 0.00/673k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38b87f12c27346069693a6ba1cebfb17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"fr/test-00000-of-00001.parquet:   0%|          | 0.00/678k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82174a4ebb94425494bf832aded5a8b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"fr/train-00000-of-00001.parquet:   0%|          | 0.00/1.34M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6a9429d138b46f69f2ad3e9fd33366a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98853fafbefe4a27ae9c46737b9bd1d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f5d3bcce4754dd88e4466364c8cdb8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3f6b636346942b8817494bee6abf53f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Flattening the indices:   0%|          | 0/6666 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29aa0e8a68ce45fe8098cb9fcb7987b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Flattening the indices:   0%|          | 0/1666 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"536bb9c6a0614d2b99bbc3565306050b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Flattening the indices:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58df1db75a484fb4b4793cac8fa4699d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Flattening the indices:   0%|          | 0/6666 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f04d7eb9a3104623a3f8ddbb5411e726"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Flattening the indices:   0%|          | 0/1666 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3592342abc994f8989ff77f357f38b1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Flattening the indices:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80745503c9d441aa91f6c089b02d0cb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Flattening the indices:   0%|          | 0/6666 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b151af4f44a546dcb48ad0d943dffbd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Flattening the indices:   0%|          | 0/1666 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04ad63a887f545c6a9a47663a516813a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Flattening the indices:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf07d17140934b7fa2d5bea7bd71e9ea"}},"metadata":{}},{"name":"stdout","text":"\nCombined dataset sizes:\n  Train: 19998\n  Validation: 4998\n  Test: 1500\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Cell 5: Initialize tokenizer and model\nprint(f\"Loading tokenizer: {config.MODEL_NAME}\")\ntokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n\nprint(f\"Loading model: {config.MODEL_NAME}\")\nmodel = AutoModelForTokenClassification.from_pretrained(\n    config.MODEL_NAME,\n    num_labels=config.NUM_LABELS,\n    id2label={i: label for i, label in enumerate(config.LABEL_NAMES)},\n    label2id={label: i for i, label in enumerate(config.LABEL_NAMES)},\n    ignore_mismatched_sizes=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:00:59.701921Z","iopub.execute_input":"2026-01-07T14:00:59.702253Z","iopub.status.idle":"2026-01-07T14:01:11.017502Z","shell.execute_reply.started":"2026-01-07T14:00:59.702225Z","shell.execute_reply":"2026-01-07T14:01:11.016892Z"}},"outputs":[{"name":"stdout","text":"Loading tokenizer: xlm-roberta-large\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e814c8b5def40c685f99aa472bb7ae8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2aca3f32ba9d4d40b064e59a555b52c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b567a6ad78684a62b66f89bdcd4fe03d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a3f77164d734901bd0e81e97f895978"}},"metadata":{}},{"name":"stdout","text":"Loading model: xlm-roberta-large\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c6dd0845be2450c914101780b490dc8"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Cell 6: Tokenization function with alignment\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"],\n        truncation=True,\n        is_split_into_words=True,\n        max_length=config.MAX_LENGTH,\n        padding=\"max_length\"\n    )\n    \n    labels = []\n    for i, label in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        \n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            else:\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n        \n        labels.append(label_ids)\n    \n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:01:15.490508Z","iopub.execute_input":"2026-01-07T14:01:15.490897Z","iopub.status.idle":"2026-01-07T14:01:15.498052Z","shell.execute_reply.started":"2026-01-07T14:01:15.490867Z","shell.execute_reply":"2026-01-07T14:01:15.497346Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Cell 7: Preprocess dataset\nprint(\"Tokenizing dataset...\")\ntokenized_dataset = dataset.map(\n    tokenize_and_align_labels,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:01:22.356669Z","iopub.execute_input":"2026-01-07T14:01:22.356979Z","iopub.status.idle":"2026-01-07T14:01:26.015084Z","shell.execute_reply.started":"2026-01-07T14:01:22.356953Z","shell.execute_reply":"2026-01-07T14:01:26.014234Z"}},"outputs":[{"name":"stdout","text":"Tokenizing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19998 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9390bcb1e3a24bcdb62c970bc1a389bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4998 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf376fe3f8404df08e0a3578678779cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a478c8c42e0143bbb0b34f91c7c3c727"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Cell 8: Metrics computation\nseqeval = load(\"seqeval\")\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n    \n    true_predictions = [\n        [config.LABEL_NAMES[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [config.LABEL_NAMES[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    results = seqeval.compute(\n        predictions=true_predictions,\n        references=true_labels\n    )\n    \n    # Log to wandb\n    wandb.log({\n        \"eval_precision\": results[\"overall_precision\"],\n        \"eval_recall\": results[\"overall_recall\"],\n        \"eval_f1\": results[\"overall_f1\"],\n        \"eval_accuracy\": results[\"overall_accuracy\"]\n    })\n    \n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"]\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:01:30.548488Z","iopub.execute_input":"2026-01-07T14:01:30.548787Z","iopub.status.idle":"2026-01-07T14:01:31.454099Z","shell.execute_reply.started":"2026-01-07T14:01:30.548761Z","shell.execute_reply":"2026-01-07T14:01:31.453534Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9acb2d5bfc2c4a19bdbfdbabd7942ae6"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Cell 9: Training arguments\ntraining_args = TrainingArguments(\n    output_dir=config.OUTPUT_DIR,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=config.LEARNING_RATE,\n    per_device_train_batch_size=config.BATCH_SIZE,\n    per_device_eval_batch_size=config.BATCH_SIZE * 2,\n    gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n    num_train_epochs=config.NUM_EPOCHS,\n    weight_decay=config.WEIGHT_DECAY,\n    warmup_ratio=config.WARMUP_RATIO,\n    logging_dir=config.LOGGING_DIR,\n    logging_steps=100,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    report_to=\"wandb\",\n    save_total_limit=2,\n    fp16=True,                             # Explicitly set to True for Kaggle GPUs (T4/P100)\n    group_by_length=True,                  # Batches sentences of similar length to reduce padding\n    push_to_hub=False,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:01:39.401996Z","iopub.execute_input":"2026-01-07T14:01:39.402589Z","iopub.status.idle":"2026-01-07T14:01:39.564027Z","shell.execute_reply.started":"2026-01-07T14:01:39.402560Z","shell.execute_reply":"2026-01-07T14:01:39.563289Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Cell 10: Data collator\ndata_collator = DataCollatorForTokenClassification(tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:01:41.369040Z","iopub.execute_input":"2026-01-07T14:01:41.369789Z","iopub.status.idle":"2026-01-07T14:01:41.374239Z","shell.execute_reply.started":"2026-01-07T14:01:41.369761Z","shell.execute_reply":"2026-01-07T14:01:41.373517Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Cell 11: Initialize trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:01:45.244097Z","iopub.execute_input":"2026-01-07T14:01:45.244865Z","iopub.status.idle":"2026-01-07T14:01:45.961395Z","shell.execute_reply.started":"2026-01-07T14:01:45.244826Z","shell.execute_reply":"2026-01-07T14:01:45.960543Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_55/1441693471.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Cell 12: Train the model\nprint(\"Starting training...\")\ntrain_result = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:01:49.649492Z","iopub.execute_input":"2026-01-07T14:01:49.649992Z","iopub.status.idle":"2026-01-07T14:44:52.000335Z","shell.execute_reply.started":"2026-01-07T14:01:49.649965Z","shell.execute_reply":"2026-01-07T14:44:51.999543Z"}},"outputs":[{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 42:56, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.245600</td>\n      <td>0.213434</td>\n      <td>0.818143</td>\n      <td>0.840700</td>\n      <td>0.829268</td>\n      <td>0.937720</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.166400</td>\n      <td>0.192309</td>\n      <td>0.840847</td>\n      <td>0.857039</td>\n      <td>0.848866</td>\n      <td>0.944120</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.123000</td>\n      <td>0.187454</td>\n      <td>0.853886</td>\n      <td>0.868709</td>\n      <td>0.861234</td>\n      <td>0.948056</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Cell 13: Save model and metrics\ntrainer.save_model()\ntokenizer.save_pretrained(config.OUTPUT_DIR)\n\n# Save training metrics\nmetrics = train_result.metrics\nmetrics.update(trainer.evaluate())\n\nwith open(os.path.join(config.OUTPUT_DIR, \"training_metrics.json\"), \"w\") as f:\n    json.dump(metrics, f, indent=2)\n\nprint(f\"Model saved to {config.OUTPUT_DIR}\")\nprint(f\"Training metrics: {metrics}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:53:52.284906Z","iopub.execute_input":"2026-01-07T14:53:52.285241Z","iopub.status.idle":"2026-01-07T14:54:58.290638Z","shell.execute_reply.started":"2026-01-07T14:53:52.285211Z","shell.execute_reply":"2026-01-07T14:54:58.290006Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='252' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [157/157 04:16]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Model saved to ./models/teacher\nTraining metrics: {'train_runtime': 2578.1083, 'train_samples_per_second': 23.271, 'train_steps_per_second': 0.727, 'total_flos': 1.3929447957815808e+16, 'train_loss': 0.2722141825358073, 'epoch': 3.0, 'eval_loss': 0.18615511059761047, 'eval_precision': 0.8538858617722971, 'eval_recall': 0.8687089715536105, 'eval_f1': 0.8612336394533227, 'eval_accuracy': 0.9480560251147065, 'eval_runtime': 60.6976, 'eval_samples_per_second': 82.343, 'eval_steps_per_second': 2.587}\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Cell 14: Evaluate on test set\nprint(\"\\nEvaluating on test set...\")\ntest_results = trainer.evaluate(tokenized_dataset[\"test\"])\nprint(f\"Test results: {test_results}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:55:16.067025Z","iopub.execute_input":"2026-01-07T14:55:16.067566Z","iopub.status.idle":"2026-01-07T14:55:34.614463Z","shell.execute_reply.started":"2026-01-07T14:55:16.067536Z","shell.execute_reply":"2026-01-07T14:55:34.613742Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating on test set...\nTest results: {'eval_loss': 0.18790274858474731, 'eval_precision': 0.844465203176086, 'eval_recall': 0.8605425987624941, 'eval_f1': 0.8524280999528524, 'eval_accuracy': 0.9450593519409689, 'eval_runtime': 18.2441, 'eval_samples_per_second': 82.218, 'eval_steps_per_second': 2.576, 'epoch': 3.0}\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Cell 15: Per-language evaluation\ndef evaluate_by_language(dataset, trainer, tokenizer):\n    language_results = {}\n    \n    # Get the list of columns to remove from the train split\n    # This prevents the ValueError because it looks at column names, not split names\n    cols_to_remove = dataset[\"train\"].column_names\n\n    for lang in config.LANGUAGES:\n        print(f\"\\nEvaluating on {lang}...\")\n        \n        # 1. Filter the DatasetDict by language\n        lang_dataset = dataset.filter(lambda x: x[\"language\"] == lang)\n        \n        # 2. Map the tokenization\n        # Use the specific column list we extracted above\n        tokenized_lang_dataset = lang_dataset.map(\n            tokenize_and_align_labels,\n            batched=True,\n            remove_columns=cols_to_remove\n        )\n        \n        # 3. Evaluate using the 'test' split of the filtered data\n        results = trainer.evaluate(tokenized_lang_dataset[\"test\"])\n        language_results[lang] = results\n        \n        print(f\"{lang} results: {results}\")\n        \n        # Log to wandb\n        wandb.log({\n            f\"{lang}/f1\": results[\"eval_f1\"],\n            f\"{lang}/precision\": results[\"eval_precision\"],\n            f\"{lang}/recall\": results[\"eval_recall\"]\n        })\n    \n    return language_results\n\n# Uncomment to run per-language evaluation\nlang_results = evaluate_by_language(dataset, trainer, tokenizer)\nlang_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:57:49.548241Z","iopub.execute_input":"2026-01-07T14:57:49.548560Z","iopub.status.idle":"2026-01-07T14:58:14.463777Z","shell.execute_reply.started":"2026-01-07T14:57:49.548533Z","shell.execute_reply":"2026-01-07T14:58:14.463268Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating on en...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6666 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bea6e1a57b94489b3ac3252f2ebd199"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1666 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1006f004dba42c5a867ac35576a7548"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14910a40589847ea98eab8eaa6a0b911"}},"metadata":{}},{"name":"stdout","text":"en results: {'eval_loss': 0.22802604734897614, 'eval_precision': 0.8235294117647058, 'eval_recall': 0.8431372549019608, 'eval_f1': 0.8332179930795848, 'eval_accuracy': 0.9333819596401653, 'eval_runtime': 6.1106, 'eval_samples_per_second': 81.825, 'eval_steps_per_second': 2.618, 'epoch': 3.0}\n\nEvaluating on de...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/19998 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"046471591e094a9aaa6c16e77284a638"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/4998 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90ab903e43bc48ff870ad804d8a1d81a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"471d843515aa46a5bdad4f62cfb6d296"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6666 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9b7895a2ac94ce18838fbddc9e7c8e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1666 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78901728508d41c6bc8a11c5d3c72af6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed07b49631304885b980fa52b785498f"}},"metadata":{}},{"name":"stdout","text":"de results: {'eval_loss': 0.1435982584953308, 'eval_precision': 0.841248303934871, 'eval_recall': 0.8635097493036211, 'eval_f1': 0.8522336769759449, 'eval_accuracy': 0.9591836734693877, 'eval_runtime': 6.116, 'eval_samples_per_second': 81.752, 'eval_steps_per_second': 2.616, 'epoch': 3.0}\n\nEvaluating on fr...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/19998 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41630e62dad4401f874d1bf72890c0ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/4998 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbcdeb47c5f94859ba535e00f17d243d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdc1b798adc8439bb91452e8c51ac72b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6666 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa4cdc3d354d42d2b3330cfe9297301e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1666 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c063de9026f465582d64a4d41b71c4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"feda89e7ebe2491085cf8403417a2264"}},"metadata":{}},{"name":"stdout","text":"fr results: {'eval_loss': 0.19990099966526031, 'eval_precision': 0.8707280832095097, 'eval_recall': 0.8759342301943199, 'eval_f1': 0.8733233979135617, 'eval_accuracy': 0.9392123287671232, 'eval_runtime': 6.0813, 'eval_samples_per_second': 82.22, 'eval_steps_per_second': 2.631, 'epoch': 3.0}\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"{'en': {'eval_loss': 0.22802604734897614,\n  'eval_precision': 0.8235294117647058,\n  'eval_recall': 0.8431372549019608,\n  'eval_f1': 0.8332179930795848,\n  'eval_accuracy': 0.9333819596401653,\n  'eval_runtime': 6.1106,\n  'eval_samples_per_second': 81.825,\n  'eval_steps_per_second': 2.618,\n  'epoch': 3.0},\n 'de': {'eval_loss': 0.1435982584953308,\n  'eval_precision': 0.841248303934871,\n  'eval_recall': 0.8635097493036211,\n  'eval_f1': 0.8522336769759449,\n  'eval_accuracy': 0.9591836734693877,\n  'eval_runtime': 6.116,\n  'eval_samples_per_second': 81.752,\n  'eval_steps_per_second': 2.616,\n  'epoch': 3.0},\n 'fr': {'eval_loss': 0.19990099966526031,\n  'eval_precision': 0.8707280832095097,\n  'eval_recall': 0.8759342301943199,\n  'eval_f1': 0.8733233979135617,\n  'eval_accuracy': 0.9392123287671232,\n  'eval_runtime': 6.0813,\n  'eval_samples_per_second': 82.22,\n  'eval_steps_per_second': 2.631,\n  'epoch': 3.0}}"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# Cell 16: Inference example (Corrected)\ndef predict_entities(text, model, tokenizer):\n    \"\"\"\n    Perform NER inference on a single text\n    \"\"\"\n    # 1. Tokenize (returns a BatchEncoding object)\n    tokens = text.split()\n    inputs = tokenizer(\n        tokens,\n        is_split_into_words=True,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=config.MAX_LENGTH\n    )\n    \n    # 2. EXTRACT WORD_IDS NOW while 'inputs' is still a BatchEncoding object\n    word_ids = inputs.word_ids(batch_index=0)\n    \n    # 3. Move to device (this turns 'inputs' into a plain dict)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    # 4. Predict\n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    predictions = torch.argmax(outputs.logits, dim=-1)[0].cpu().numpy()\n    \n    # 5. Align predictions with tokens\n    # (word_ids is already defined from step 2)\n    previous_word_idx = None\n    predictions_aligned = []\n    \n    for idx, word_idx in enumerate(word_ids):\n        if word_idx is not None and word_idx != previous_word_idx:\n            predictions_aligned.append(predictions[idx])\n        previous_word_idx = word_idx\n        \n    # ... (rest of your label conversion logic is fine)\n    # Convert to labels\n    entities = []\n    current_entity = None\n    current_start = None\n    \n    for i, (token, pred_idx) in enumerate(zip(tokens, predictions_aligned)):\n        label = config.LABEL_NAMES[pred_idx]\n        \n        if label.startswith(\"B-\"):\n            if current_entity:\n                entities.append({\n                    \"entity\": \" \".join(tokens[current_start:i]),\n                    \"label\": current_entity,\n                    \"start\": current_start,\n                    \"end\": i\n                })\n            current_entity = label[2:]\n            current_start = i\n        elif label.startswith(\"I-\") and current_entity == label[2:]:\n            continue\n        else:\n            if current_entity:\n                entities.append({\n                    \"entity\": \" \".join(tokens[current_start:i]),\n                    \"label\": current_entity,\n                    \"start\": current_start,\n                    \"end\": i\n                })\n                current_entity = None\n                current_start = None\n    \n    if current_entity:\n        entities.append({\n            \"entity\": \" \".join(tokens[current_start:]),\n            \"label\": current_entity,\n            \"start\": current_start,\n            \"end\": len(tokens)\n        })\n    \n    return entities\n\n# Test inference\ntest_text = \"Apple Inc. is headquartered in Cupertino, California and was founded by Steve Jobs.\"\nprint(f\"\\nText: {test_text}\")\nentities = predict_entities(test_text, model, tokenizer)\nprint(f\"Entities found: {entities}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:59:19.741482Z","iopub.execute_input":"2026-01-07T14:59:19.742212Z","iopub.status.idle":"2026-01-07T14:59:19.784584Z","shell.execute_reply.started":"2026-01-07T14:59:19.742185Z","shell.execute_reply":"2026-01-07T14:59:19.784028Z"}},"outputs":[{"name":"stdout","text":"\nText: Apple Inc. is headquartered in Cupertino, California and was founded by Steve Jobs.\nEntities found: [{'entity': 'Apple Inc.', 'label': 'ORG', 'start': 0, 'end': 2}, {'entity': 'Cupertino, California', 'label': 'LOC', 'start': 5, 'end': 7}, {'entity': 'Steve Jobs.', 'label': 'PER', 'start': 11, 'end': 13}]\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Cell 17: Save model artifacts\ndef save_model_artifacts(model, tokenizer, config, output_dir):\n    # Save model in different formats\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    \n    # Save config\n    config_dict = {\n        \"model_name\": config.MODEL_NAME,\n        \"languages\": config.LANGUAGES,\n        \"max_length\": config.MAX_LENGTH,\n        \"label_names\": config.LABEL_NAMES,\n        \"training_params\": {\n            \"batch_size\": config.BATCH_SIZE,\n            \"learning_rate\": config.LEARNING_RATE,\n            \"epochs\": config.NUM_EPOCHS\n        }\n    }\n    \n    with open(os.path.join(output_dir, \"config.json\"), \"w\") as f:\n        json.dump(config_dict, f, indent=2)\n    \n    print(f\"Model artifacts saved to {output_dir}\")\n\nsave_model_artifacts(model, tokenizer, config, config.OUTPUT_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:59:29.251958Z","iopub.execute_input":"2026-01-07T14:59:29.252587Z","iopub.status.idle":"2026-01-07T14:59:38.541628Z","shell.execute_reply.started":"2026-01-07T14:59:29.252559Z","shell.execute_reply":"2026-01-07T14:59:38.540856Z"}},"outputs":[{"name":"stdout","text":"Model artifacts saved to ./models/teacher\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import json\nimport os\nimport matplotlib.pyplot as plt\n\ndef plot_training_metrics(state_file):\n    # Load the trainer_state.json which contains the history\n    with open(state_file, 'r') as f:\n        state = json.load(f)\n    \n    history = state['log_history']\n    \n    # Extract values for plotting\n    train_loss = [x['loss'] for x in history if 'loss' in x]\n    train_steps = [x['step'] for x in history if 'loss' in x]\n    \n    eval_f1 = [x['eval_f1'] for x in history if 'eval_f1' in x]\n    eval_steps = [x['step'] for x in history if 'eval_f1' in x]\n    \n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Plot Training Loss\n    if train_loss:\n        axes[0].plot(train_steps, train_loss, label='Training Loss', color='blue')\n        axes[0].set_title('Training Loss over Steps')\n        axes[0].set_xlabel('Step')\n        axes[0].set_ylabel('Loss')\n        axes[0].grid(True)\n    \n    # Plot Validation F1\n    if eval_f1:\n        axes[1].plot(eval_steps, eval_f1, label='Eval F1', color='green', marker='o')\n        axes[1].set_title('Validation F1 Score')\n        axes[1].set_xlabel('Step')\n        axes[1].set_ylabel('F1 Score')\n        axes[1].grid(True)\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(config.OUTPUT_DIR, \"training_plots.png\"))\n    plt.show()\n\n# Point this to the trainer_state.json in your output directory\nplot_training_metrics('/kaggle/working/models/teacher/checkpoint-1875/trainer_state.json')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:04:09.875579Z","iopub.execute_input":"2026-01-07T15:04:09.875881Z","iopub.status.idle":"2026-01-07T15:04:10.340649Z","shell.execute_reply.started":"2026-01-07T15:04:09.875855Z","shell.execute_reply":"2026-01-07T15:04:10.340041Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x500 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# Cell 19: Complete W&B run\nwandb.finish()\nprint(\"\\nTraining completed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:04:36.978592Z","iopub.execute_input":"2026-01-07T15:04:36.979274Z","iopub.status.idle":"2026-01-07T15:04:36.999512Z","shell.execute_reply.started":"2026-01-07T15:04:36.979243Z","shell.execute_reply":"2026-01-07T15:04:36.998822Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>de/f1</td><td>▁</td></tr><tr><td>de/precision</td><td>▁</td></tr><tr><td>de/recall</td><td>▁</td></tr><tr><td>en/f1</td><td>▁</td></tr><tr><td>en/precision</td><td>▁</td></tr><tr><td>en/recall</td><td>▁</td></tr><tr><td>eval/accuracy</td><td>▂▄▅▅▄▁█▃</td></tr><tr><td>eval/f1</td><td>▁▄▆▆▅▂▅█</td></tr><tr><td>eval/loss</td><td>▇▅▅▅▅█▁▆</td></tr><tr><td>eval/precision</td><td>▁▄▆▆▅▂▄█</td></tr><tr><td>+16</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>de/f1</td><td>0.85223</td></tr><tr><td>de/precision</td><td>0.84125</td></tr><tr><td>de/recall</td><td>0.86351</td></tr><tr><td>en/f1</td><td>0.83322</td></tr><tr><td>en/precision</td><td>0.82353</td></tr><tr><td>en/recall</td><td>0.84314</td></tr><tr><td>eval/accuracy</td><td>0.93921</td></tr><tr><td>eval/f1</td><td>0.87332</td></tr><tr><td>eval/loss</td><td>0.1999</td></tr><tr><td>eval/precision</td><td>0.87073</td></tr><tr><td>+21</td><td>...</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"You can sync this run to the cloud by running:<br><code>wandb sync /kaggle/working/wandb/offline-run-20260107_140021-u7cbf6bb<code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/offline-run-20260107_140021-u7cbf6bb/logs</code>"},"metadata":{}},{"name":"stdout","text":"\nTraining completed successfully!\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"!# To remove the folder and all files inside (free up space)\n!rm -rf /kaggle/working/models/teacher/checkpoint-1875","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:18:20.130664Z","iopub.execute_input":"2026-01-07T15:18:20.131563Z","iopub.status.idle":"2026-01-07T15:18:21.363591Z","shell.execute_reply.started":"2026-01-07T15:18:20.131527Z","shell.execute_reply":"2026-01-07T15:18:21.362833Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"!rm -rf /kaggle/working/teacher_model_training_compressed.tar.xz","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:18:54.653586Z","iopub.execute_input":"2026-01-07T15:18:54.654405Z","iopub.status.idle":"2026-01-07T15:18:54.855479Z","shell.execute_reply.started":"2026-01-07T15:18:54.654370Z","shell.execute_reply":"2026-01-07T15:18:54.854636Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"!tar -cvJf /kaggle/working/teacher_model_training_compressed.tar.xz -C /kaggle/working . --exclude=teacher_model_training_compressed.tar.xz","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:20:19.154044Z","iopub.execute_input":"2026-01-07T15:20:19.154816Z","iopub.status.idle":"2026-01-07T15:29:03.319655Z","shell.execute_reply.started":"2026-01-07T15:20:19.154783Z","shell.execute_reply":"2026-01-07T15:29:03.318662Z"}},"outputs":[{"name":"stdout","text":"./\n./models/\n./models/teacher/\n./models/teacher/config.json\n./models/teacher/training_metrics.json\n./models/teacher/special_tokens_map.json\n./models/teacher/tokenizer.json\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"./models/teacher/tokenizer_config.json\n./models/teacher/training_plots.png\n./models/teacher/model.safetensors\n^C\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"# STUDENT","metadata":{}},{"cell_type":"code","source":"# notebooks/03_knowledge_distillation.ipynb\n\n# Cell 1: Setup and imports\n#!pip install transformers[torch] datasets accelerate\n#!pip install torch --index-url https://download.pytorch.org/whl/cu118\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForTokenClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForTokenClassification\n)\nfrom transformers.trainer_utils import EvalPrediction\nimport os\nimport json\nfrom typing import Optional, Dict, Any\nimport wandb\n\n# Set seeds\ndef set_seed(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nset_seed(42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:34:00.570646Z","iopub.execute_input":"2026-01-07T15:34:00.571407Z","iopub.status.idle":"2026-01-07T15:34:00.579036Z","shell.execute_reply.started":"2026-01-07T15:34:00.571373Z","shell.execute_reply":"2026-01-07T15:34:00.578443Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Cell 2: Configuration for distillation\nclass DistillationConfig:\n    TEACHER_MODEL = \"./models/teacher\"  # Path to trained teacher\n    STUDENT_MODEL = \"xlm-roberta-base\"  # Smaller model\n    DATASET_NAME = \"wikiann\"\n    LANGUAGES = [\"en\", \"de\", \"fr\"]\n    MAX_LENGTH = 128\n    BATCH_SIZE = 32  # Larger batch for distillation\n    GRADIENT_ACCUMULATION_STEPS = 1\n    LEARNING_RATE = 3e-5\n    NUM_EPOCHS = 5\n    TEMPERATURE = 2.0  # Temperature for softmax\n    ALPHA = 0.5  # Weight for distillation loss vs hard labels\n    OUTPUT_DIR = \"./models/student_distilled\"\n    LABEL_NAMES = [\n        \"O\",\n        \"B-PER\", \"I-PER\",\n        \"B-ORG\", \"I-ORG\",\n        \"B-LOC\", \"I-LOC\"\n    ]\n    NUM_LABELS = 7\n\nconfig = DistillationConfig()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:34:04.607690Z","iopub.execute_input":"2026-01-07T15:34:04.607988Z","iopub.status.idle":"2026-01-07T15:34:04.612903Z","shell.execute_reply.started":"2026-01-07T15:34:04.607961Z","shell.execute_reply":"2026-01-07T15:34:04.612193Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Cell 3: Initialize W&B\nwandb.init(\n    project=\"multilingual-ner\",\n    name=\"knowledge-distillation\",\n    mode=\"offline\",              # offline mode\n    config={\n        \"teacher\": config.TEACHER_MODEL,\n        \"student\": config.STUDENT_MODEL,\n        \"temperature\": config.TEMPERATURE,\n        \"alpha\": config.ALPHA,\n        \"batch_size\": config.BATCH_SIZE,\n        \"learning_rate\": config.LEARNING_RATE,\n        \"epochs\": config.NUM_EPOCHS\n    }\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:34:26.126863Z","iopub.execute_input":"2026-01-07T15:34:26.127201Z","iopub.status.idle":"2026-01-07T15:34:32.538634Z","shell.execute_reply.started":"2026-01-07T15:34:26.127173Z","shell.execute_reply":"2026-01-07T15:34:32.537895Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.22.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/kaggle/working/wandb/offline-run-20260107_153426-lh1y9gxk</code>"},"metadata":{}},{"execution_count":34,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7a46a64177a0>"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"# Cell 4: Load teacher model\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer, XLMRobertaConfig\n\nprint(\"Loading teacher model...\")\n\n# 1. Manually create the architecture config\n# This tells the library: \"Even if the JSON is custom, treat this as an XLM-RoBERTa model with 7 labels\"\nmanual_config = XLMRobertaConfig.from_pretrained(\n    \"xlm-roberta-large\", \n    num_labels=len(config.LABEL_NAMES),\n    id2label={i: label for i, label in enumerate(config.LABEL_NAMES)},\n    label2id={label: i for i, label in enumerate(config.LABEL_NAMES)}\n)\n\n# 2. Load the tokenizer from the local folder\nteacher_tokenizer = AutoTokenizer.from_pretrained(config.TEACHER_MODEL)\n\n# 3. Load the model using the manual_config + your saved weights\n# This maps your model.safetensors onto the XLM-R Large architecture\nteacher_model = AutoModelForTokenClassification.from_pretrained(\n    config.TEACHER_MODEL,\n    config=manual_config\n)\n\nteacher_model.eval()\nprint(\"Teacher model loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:39:54.281620Z","iopub.execute_input":"2026-01-07T15:39:54.282401Z","iopub.status.idle":"2026-01-07T15:39:55.416541Z","shell.execute_reply.started":"2026-01-07T15:39:54.282372Z","shell.execute_reply":"2026-01-07T15:39:55.415756Z"}},"outputs":[{"name":"stdout","text":"Loading teacher model...\nTeacher model loaded successfully!\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# Cell 5: Load student model\nprint(\"Loading student model...\")\nstudent_tokenizer = AutoTokenizer.from_pretrained(config.STUDENT_MODEL)\nstudent_model = AutoModelForTokenClassification.from_pretrained(\n    config.STUDENT_MODEL,\n    num_labels=config.NUM_LABELS,\n    id2label={i: label for i, label in enumerate(config.LABEL_NAMES)},\n    label2id={label: i for i, label in enumerate(config.LABEL_NAMES)}\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:40:17.184611Z","iopub.execute_input":"2026-01-07T15:40:17.184915Z","iopub.status.idle":"2026-01-07T15:40:25.166405Z","shell.execute_reply.started":"2026-01-07T15:40:17.184889Z","shell.execute_reply":"2026-01-07T15:40:25.165731Z"}},"outputs":[{"name":"stdout","text":"Loading student model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb239691c66b48e8a23ebb2385df995f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"752fcfd8d5ba4437aaecf7205041dd1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f424a18534a47b3b2389880919b07f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd584de2409d454c9ae6da0c7644aa83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a44ae6bbc69e4d8a9a0a8f9f4451dddd"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# Cell 4: Load and prepare multilingual dataset\n\ndef load_multilingual_dataset(languages, max_train_samples=20000):\n    datasets_dict = {}\n    \n    # 1. Load individual datasets from Hugging Face\n    for lang in languages:\n        try:\n            print(f\"Loading {lang} dataset...\")\n            datasets_dict[lang] = load_dataset(config.DATASET_NAME, lang)\n        except Exception as e:\n            print(f\"Error loading {lang}: {e}\")\n    \n    train_list, val_list, test_list = [], [], []\n    \n    # 2. Calculate how many samples to take per language to be fair\n    # This ensures an even distribution across your chosen languages\n    samples_per_lang = max_train_samples // len(languages)\n    eval_samples_per_lang = samples_per_lang // 4  # Keep validation smaller for speed\n\n    for lang, ds in datasets_dict.items():\n        # Shuffle and select a subset for each language to keep it representative\n        train_sub = ds['train'].shuffle(seed=42).select(range(min(samples_per_lang, len(ds['train']))))\n        val_sub = ds['validation'].shuffle(seed=42).select(range(min(eval_samples_per_lang, len(ds['validation']))))\n        \n        # Select a small portion of the test set for the specific language\n        test_sub = ds['test'].shuffle(seed=42).select(range(min(500, len(ds['test']))))\n        \n        # Add 'language' column to ALL splits to avoid ValueError during tokenization (.map)\n        train_list.append(train_sub.add_column(\"language\", [lang] * len(train_sub)))\n        val_list.append(val_sub.add_column(\"language\", [lang] * len(val_sub)))\n        test_list.append(test_sub.add_column(\"language\", [lang] * len(test_sub)))\n        \n    # 3. Combine everything into a single DatasetDict\n    # Using concatenate_datasets keeps the object as a Hugging Face Dataset (not a list)\n    combined_dataset = DatasetDict({\n        'train': concatenate_datasets(train_list).shuffle(seed=42),\n        'validation': concatenate_datasets(val_list).shuffle(seed=42),\n        'test': concatenate_datasets(test_list).shuffle(seed=42)\n    })\n    \n    # Print statistics for verification\n    print(f\"\\nCombined dataset sizes:\")\n    print(f\"  Train: {len(combined_dataset['train'])}\")\n    print(f\"  Validation: {len(combined_dataset['validation'])}\")\n    print(f\"  Test: {len(combined_dataset['test'])}\")\n    \n    return combined_dataset\n\n# Run the loader\ndataset = load_multilingual_dataset(config.LANGUAGES)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:41:24.474077Z","iopub.execute_input":"2026-01-07T15:41:24.474885Z","iopub.status.idle":"2026-01-07T15:41:30.595185Z","shell.execute_reply.started":"2026-01-07T15:41:24.474845Z","shell.execute_reply":"2026-01-07T15:41:30.594440Z"}},"outputs":[{"name":"stdout","text":"Loading en dataset...\nLoading de dataset...\nLoading fr dataset...\n\nCombined dataset sizes:\n  Train: 19998\n  Validation: 4998\n  Test: 1500\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# Cell 7: Tokenization function\ndef tokenize_and_align_labels(examples, tokenizer):\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"],\n        truncation=True,\n        is_split_into_words=True,\n        max_length=config.MAX_LENGTH,\n        padding=\"max_length\"\n    )\n    \n    labels = []\n    for i, label in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        \n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            else:\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n        \n        labels.append(label_ids)\n    \n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:42:27.518211Z","iopub.execute_input":"2026-01-07T15:42:27.518534Z","iopub.status.idle":"2026-01-07T15:42:27.526367Z","shell.execute_reply.started":"2026-01-07T15:42:27.518507Z","shell.execute_reply":"2026-01-07T15:42:27.525656Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# Cell 8: Custom distillation trainer\nclass DistillationTrainer(Trainer):\n    def __init__(self, *args, teacher_model=None, temperature=2.0, alpha=0.5, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.teacher_model = teacher_model\n        self.temperature = temperature\n        self.alpha = alpha\n        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n        \n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        \"\"\"\n        Updated signature to include num_items_in_batch for transformers compatibility\n        \"\"\"\n        labels = inputs.pop(\"labels\")\n        \n        # Get student outputs\n        outputs = model(**inputs)\n        student_logits = outputs.logits\n        \n        # Get teacher outputs (no gradient)\n        with torch.no_grad():\n            teacher_outputs = self.teacher_model(**inputs)\n            teacher_logits = teacher_outputs.logits\n        \n        # Compute distillation loss (KL divergence)\n        # We divide by T^2 as per Hintons paper to keep gradients consistent\n        distillation_loss = self.kl_loss(\n            F.log_softmax(student_logits / self.temperature, dim=-1),\n            F.softmax(teacher_logits / self.temperature, dim=-1)\n        ) * (self.temperature ** 2)\n        \n        # Compute hard label loss (standard CrossEntropy)\n        loss_fct = nn.CrossEntropyLoss()\n        hard_label_loss = loss_fct(\n            student_logits.view(-1, model.config.num_labels),\n            labels.view(-1)\n        )\n        \n        # Combined loss: α knowledge from teacher + (1-α) ground truth\n        loss = self.alpha * distillation_loss + (1 - self.alpha) * hard_label_loss\n        \n        # Note: In newer Trainer versions, don't manually log to wandb inside compute_loss\n        # as it runs every step and can slow down training. \n        # The Trainer handles logging via its own logging_steps logic.\n\n        return (loss, outputs) if return_outputs else loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:45:17.891766Z","iopub.execute_input":"2026-01-07T15:45:17.892130Z","iopub.status.idle":"2026-01-07T15:45:17.901454Z","shell.execute_reply.started":"2026-01-07T15:45:17.892097Z","shell.execute_reply":"2026-01-07T15:45:17.900820Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"# Cell 9: Preprocess dataset for student\nprint(\"Tokenizing dataset for student...\")\ntokenized_dataset = dataset.map(\n    lambda x: tokenize_and_align_labels(x, student_tokenizer),\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:42:34.990362Z","iopub.execute_input":"2026-01-07T15:42:34.991051Z","iopub.status.idle":"2026-01-07T15:42:39.802920Z","shell.execute_reply.started":"2026-01-07T15:42:34.991010Z","shell.execute_reply":"2026-01-07T15:42:39.802125Z"}},"outputs":[{"name":"stdout","text":"Tokenizing dataset for student...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19998 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42c0e1930fe7499f8f89c6b8ad31601f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4998 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57e829b23c694769bc57552c13a4fb6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fe25d1d90044584871034c639cc2b97"}},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"# Cell 10: Training arguments for distillation\ntraining_args = TrainingArguments(\n    output_dir=config.OUTPUT_DIR,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=config.LEARNING_RATE,\n    per_device_train_batch_size=config.BATCH_SIZE,\n    per_device_eval_batch_size=config.BATCH_SIZE * 2,\n    num_train_epochs=config.NUM_EPOCHS,\n    weight_decay=0.01,\n    warmup_ratio=0.1,\n    logging_dir=\"./logs/distillation\",\n    logging_steps=50,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"wandb\",\n    save_total_limit=2,\n    fp16=True,\n    push_to_hub=False,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:43:08.315301Z","iopub.execute_input":"2026-01-07T15:43:08.316141Z","iopub.status.idle":"2026-01-07T15:43:08.350942Z","shell.execute_reply.started":"2026-01-07T15:43:08.316100Z","shell.execute_reply":"2026-01-07T15:43:08.350173Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# Cell 11: Compute metrics function\nfrom evaluate import load\nseqeval = load(\"seqeval\")\n\ndef compute_metrics(p: EvalPrediction):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n    \n    true_predictions = [\n        [config.LABEL_NAMES[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [config.LABEL_NAMES[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    results = seqeval.compute(\n        predictions=true_predictions,\n        references=true_labels\n    )\n    \n    wandb.log({\n        \"eval_precision\": results[\"overall_precision\"],\n        \"eval_recall\": results[\"overall_recall\"],\n        \"eval_f1\": results[\"overall_f1\"],\n        \"eval_accuracy\": results[\"overall_accuracy\"]\n    })\n    \n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"]\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:43:13.428028Z","iopub.execute_input":"2026-01-07T15:43:13.428366Z","iopub.status.idle":"2026-01-07T15:43:14.157439Z","shell.execute_reply.started":"2026-01-07T15:43:13.428332Z","shell.execute_reply":"2026-01-07T15:43:14.156759Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# Cell 12: Data collator\ndata_collator = DataCollatorForTokenClassification(student_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:43:17.316028Z","iopub.execute_input":"2026-01-07T15:43:17.316843Z","iopub.status.idle":"2026-01-07T15:43:17.321431Z","shell.execute_reply.started":"2026-01-07T15:43:17.316814Z","shell.execute_reply":"2026-01-07T15:43:17.320787Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# Cell 13: Create distillation trainer\ntrainer = DistillationTrainer(\n    model=student_model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    tokenizer=student_tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    teacher_model=teacher_model,\n    temperature=config.TEMPERATURE,\n    alpha=config.ALPHA\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:45:32.147834Z","iopub.execute_input":"2026-01-07T15:45:32.148464Z","iopub.status.idle":"2026-01-07T15:45:32.162839Z","shell.execute_reply.started":"2026-01-07T15:45:32.148434Z","shell.execute_reply":"2026-01-07T15:45:32.162129Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_55/3841941174.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `DistillationTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(*args, **kwargs)\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"# Move teacher to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nteacher_model.to(device)\n\n# Verify student is also there\nstudent_model.to(device)\n\nprint(f\"Teacher device: {teacher_model.device}\")\nprint(f\"Student device: {student_model.device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:46:22.690418Z","iopub.execute_input":"2026-01-07T15:46:22.691038Z","iopub.status.idle":"2026-01-07T15:46:23.418907Z","shell.execute_reply.started":"2026-01-07T15:46:22.691010Z","shell.execute_reply":"2026-01-07T15:46:23.418280Z"}},"outputs":[{"name":"stdout","text":"Teacher device: cuda:0\nStudent device: cuda:0\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"# Cell 14: Train the student model\nprint(\"Starting knowledge distillation...\")\ntrain_result = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:46:35.661204Z","iopub.execute_input":"2026-01-07T15:46:35.661502Z","iopub.status.idle":"2026-01-07T16:32:34.367920Z","shell.execute_reply.started":"2026-01-07T15:46:35.661476Z","shell.execute_reply":"2026-01-07T16:32:34.366630Z"}},"outputs":[{"name":"stdout","text":"Starting knowledge distillation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3125/3125 45:57, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>14.171600</td>\n      <td>10.333790</td>\n      <td>0.775250</td>\n      <td>0.790518</td>\n      <td>0.782810</td>\n      <td>0.917629</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>9.365900</td>\n      <td>8.382884</td>\n      <td>0.796489</td>\n      <td>0.814150</td>\n      <td>0.805223</td>\n      <td>0.930717</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>7.052600</td>\n      <td>6.768042</td>\n      <td>0.819398</td>\n      <td>0.830635</td>\n      <td>0.824978</td>\n      <td>0.935330</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>5.242900</td>\n      <td>6.313475</td>\n      <td>0.822241</td>\n      <td>0.838074</td>\n      <td>0.830082</td>\n      <td>0.938083</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>4.239300</td>\n      <td>6.183316</td>\n      <td>0.818932</td>\n      <td>0.839241</td>\n      <td>0.828963</td>\n      <td>0.937841</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"# Cell 15: Save distilled model\ntrainer.save_model()\nstudent_tokenizer.save_pretrained(config.OUTPUT_DIR)\n\n# Save distillation metrics\nmetrics = train_result.metrics\nmetrics.update(trainer.evaluate())\n\nwith open(os.path.join(config.OUTPUT_DIR, \"distillation_metrics.json\"), \"w\") as f:\n    json.dump(metrics, f, indent=2)\n\nprint(f\"Distilled model saved to {config.OUTPUT_DIR}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:32:54.102022Z","iopub.execute_input":"2026-01-07T16:32:54.102605Z","iopub.status.idle":"2026-01-07T16:34:14.862461Z","shell.execute_reply.started":"2026-01-07T16:32:54.102571Z","shell.execute_reply":"2026-01-07T16:34:14.861820Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [79/79 01:16]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Distilled model saved to ./models/student_distilled\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"# Cell 16: Compare teacher vs student\ndef compare_models(teacher_model, student_model, tokenizer, dataset, num_samples=100):\n    \"\"\"\n    Compare predictions between teacher and student\n    \"\"\"\n    import random\n    import torch\n    import numpy as np\n    \n    # Get random samples from validation set\n    samples = random.sample(list(dataset[\"validation\"]), num_samples)\n    \n    teacher_correct = 0\n    student_correct = 0\n    agreement = 0\n    total_tokens = 0\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    teacher_model.to(device)\n    student_model.to(device)\n    \n    # Set both models to evaluation mode\n    teacher_model.eval()\n    student_model.eval()\n    \n    for sample in samples:\n        tokens = sample[\"tokens\"]\n        true_labels = sample[\"ner_tags\"]\n        \n        # 1. Tokenize (returns a BatchEncoding object)\n        # We need this object to access .word_ids()\n        inputs_obj = tokenizer(\n            tokens,\n            is_split_into_words=True,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=config.MAX_LENGTH,\n            padding=\"max_length\"\n        )\n        \n        # 2. Extract word_ids IMMEDIATELY\n        # This is the fix for the AttributeError\n        word_ids = inputs_obj.word_ids(batch_index=0)\n        \n        # 3. Move tensors to device (converts inputs_obj to a standard dict)\n        inputs = {k: v.to(device) for k, v in inputs_obj.items()}\n        \n        # 4. Get predictions\n        with torch.no_grad():\n            teacher_outputs = teacher_model(**inputs)\n            student_outputs = student_model(**inputs)\n        \n        # Move logits to CPU and get the max arg\n        teacher_preds = torch.argmax(teacher_outputs.logits, dim=-1)[0].cpu().numpy()\n        student_preds = torch.argmax(student_outputs.logits, dim=-1)[0].cpu().numpy()\n        \n        # 5. Align predictions with words\n        previous_word_idx = None\n        teacher_aligned = []\n        student_aligned = []\n        \n        for idx, word_idx in enumerate(word_ids):\n            # Only look at the first token of each word (standard NER evaluation)\n            if word_idx is not None and word_idx != previous_word_idx:\n                if word_idx < len(true_labels):  # Safety check for bounds\n                    teacher_aligned.append(teacher_preds[idx])\n                    student_aligned.append(student_preds[idx])\n            previous_word_idx = word_idx\n        \n        # 6. Compare aligned predictions with Ground Truth\n        # Ensure we only compare if lengths match exactly\n        if len(teacher_aligned) == len(true_labels):\n            teacher_correct += sum(1 for t, tl in zip(teacher_aligned, true_labels) if t == tl)\n            student_correct += sum(1 for s, tl in zip(student_aligned, true_labels) if s == tl)\n            agreement += sum(1 for t, s in zip(teacher_aligned, student_aligned) if t == s)\n            total_tokens += len(true_labels)\n    \n    # Avoid division by zero\n    total_tokens = max(total_tokens, 1)\n    \n    return {\n        \"teacher_accuracy\": teacher_correct / total_tokens,\n        \"student_accuracy\": student_correct / total_tokens,\n        \"prediction_agreement\": agreement / total_tokens,\n        \"total_tokens_evaluated\": total_tokens\n    }\n\n# Run comparison\nprint(\"Running comparison between Teacher and Student...\")\ncomparison = compare_models(teacher_model, student_model, student_tokenizer, dataset)\n\nprint(f\"\\nModel comparison results ({comparison['total_tokens_evaluated']} tokens):\")\nprint(f\"  Teacher accuracy:      {comparison['teacher_accuracy']:.4f}\")\nprint(f\"  Student accuracy:      {comparison['student_accuracy']:.4f}\")\nprint(f\"  Prediction agreement: {comparison['prediction_agreement']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:35:39.856445Z","iopub.execute_input":"2026-01-07T16:35:39.856752Z","iopub.status.idle":"2026-01-07T16:35:43.838170Z","shell.execute_reply.started":"2026-01-07T16:35:39.856725Z","shell.execute_reply":"2026-01-07T16:35:43.837452Z"}},"outputs":[{"name":"stdout","text":"Running comparison between Teacher and Student...\n\nModel comparison results (989 tokens):\n  Teacher accuracy:      0.9616\n  Student accuracy:      0.9505\n  Prediction agreement: 0.9767\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"# Cell 17: Compute model sizes\ndef get_model_size(model, model_name=\"\"):\n    param_size = 0\n    for param in model.parameters():\n        param_size += param.nelement() * param.element_size()\n    buffer_size = 0\n    for buffer in model.buffers():\n        buffer_size += buffer.nelement() * buffer.element_size()\n    \n    size_all_mb = (param_size + buffer_size) / 1024**2\n    return size_all_mb\n\nteacher_size = get_model_size(teacher_model, \"Teacher\")\nstudent_size = get_model_size(student_model, \"Student\")\n\nprint(f\"\\nModel sizes:\")\nprint(f\"  Teacher model: {teacher_size:.2f} MB\")\nprint(f\"  Student model: {student_size:.2f} MB\")\nprint(f\"  Compression ratio: {teacher_size/student_size:.2f}x\")\n\n# Log to wandb\nwandb.log({\n    \"teacher_size_mb\": teacher_size,\n    \"student_size_mb\": student_size,\n    \"compression_ratio\": teacher_size/student_size,\n    \"teacher_accuracy\": comparison['teacher_accuracy'],\n    \"student_accuracy\": comparison['student_accuracy']\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:35:57.424181Z","iopub.execute_input":"2026-01-07T16:35:57.424465Z","iopub.status.idle":"2026-01-07T16:35:57.437920Z","shell.execute_reply.started":"2026-01-07T16:35:57.424442Z","shell.execute_reply":"2026-01-07T16:35:57.437287Z"}},"outputs":[{"name":"stdout","text":"\nModel sizes:\n  Teacher model: 2131.84 MB\n  Student model: 1058.43 MB\n  Compression ratio: 2.01x\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"# Cell 18: Save comparison results\ncomparison_results = {\n    \"model_sizes\": {\n        \"teacher_mb\": teacher_size,\n        \"student_mb\": student_size,\n        \"compression_ratio\": teacher_size/student_size\n    },\n    \"performance\": comparison,\n    \"distillation_params\": {\n        \"temperature\": config.TEMPERATURE,\n        \"alpha\": config.ALPHA\n    }\n}\n\nwith open(os.path.join(config.OUTPUT_DIR, \"comparison_results.json\"), \"w\") as f:\n    json.dump(comparison_results, f, indent=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:36:08.668201Z","iopub.execute_input":"2026-01-07T16:36:08.668785Z","iopub.status.idle":"2026-01-07T16:36:08.674361Z","shell.execute_reply.started":"2026-01-07T16:36:08.668756Z","shell.execute_reply":"2026-01-07T16:36:08.673752Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"# Cell 19: Visualize knowledge transfer\ndef plot_knowledge_transfer(state_file):\n    \"\"\"\n    Parses trainer_state.json to visualize the distillation process\n    \"\"\"\n    # Load training logs\n    try:\n        with open(state_file, 'r') as f:\n            state = json.load(f)\n        \n        # Access the history list\n        history = state.get('log_history', [])\n        \n        # Extract data for plotting\n        steps = []\n        total_loss = []\n        distill_loss = []\n        hard_loss = []\n        \n        eval_steps = []\n        eval_f1 = []\n\n        for entry in history:\n            # Training logs usually have 'loss'\n            if 'loss' in entry:\n                steps.append(entry['step'])\n                total_loss.append(entry['loss'])\n                # These might be present if you used custom logging in compute_loss\n                if 'distillation_loss' in entry:\n                    distill_loss.append(entry['distillation_loss'])\n                if 'hard_label_loss' in entry:\n                    hard_loss.append(entry['hard_label_loss'])\n            \n            # Evaluation logs usually have 'eval_f1' or 'eval_loss'\n            if 'eval_f1' in entry:\n                eval_steps.append(entry['step'])\n                eval_f1.append(entry['eval_f1'])\n\n        # Create visualization\n        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n        \n        # Plot 1: Total Training Loss\n        if total_loss:\n            axes[0].plot(steps, total_loss, label='Total Loss', color='blue')\n            axes[0].set_title('Student Training Loss')\n            axes[0].set_xlabel('Step')\n            axes[0].set_ylabel('Loss')\n            axes[0].grid(True, linestyle='--', alpha=0.6)\n            \n            # Add component losses if they exist\n            if distill_loss:\n                axes[0].plot(steps, distill_loss, label='Distill Component', alpha=0.7)\n            if hard_loss:\n                axes[0].plot(steps, hard_loss, label='Hard Label Component', alpha=0.7)\n            axes[0].legend()\n        \n        # Plot 2: Validation F1 Score\n        if eval_f1:\n            axes[1].plot(eval_steps, eval_f1, marker='o', color='green', label='Student F1')\n            axes[1].set_title('Student Validation F1 Score')\n            axes[1].set_xlabel('Step')\n            axes[1].set_ylabel('F1 Score')\n            axes[1].grid(True, linestyle='--', alpha=0.6)\n            axes[1].legend()\n\n        plt.tight_layout()\n        \n        # Ensure output directory exists before saving\n        os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n        save_path = os.path.join(config.OUTPUT_DIR, \"distillation_plots.png\")\n        plt.savefig(save_path)\n        print(f\"Plots saved to {save_path}\")\n        plt.show()\n        \n    except FileNotFoundError:\n        print(f\"Error: The file {state_file} was not found.\")\n    except Exception as e:\n        print(f\"Could not generate plots: {e}\")\n\n# Call the function with your specific checkpoint path\nplot_knowledge_transfer('/kaggle/working/models/student_distilled/checkpoint-3125/trainer_state.json')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:38:43.997349Z","iopub.execute_input":"2026-01-07T16:38:43.998182Z","iopub.status.idle":"2026-01-07T16:38:44.521042Z","shell.execute_reply.started":"2026-01-07T16:38:43.998150Z","shell.execute_reply":"2026-01-07T16:38:44.520359Z"}},"outputs":[{"name":"stdout","text":"Plots saved to ./models/student_distilled/distillation_plots.png\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x600 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"# Cell 20: Test inference with distilled model\ndef test_distilled_inference(model, tokenizer, text):\n    print(f\"\\nTesting inference with distilled model:\")\n    print(f\"Text: {text}\")\n    \n    # 1. Tokenize as a list of words\n    tokens = text.split()\n    inputs_obj = tokenizer(\n        tokens,\n        is_split_into_words=True,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=config.MAX_LENGTH\n    )\n    \n    # 2. Extract word_ids BEFORE moving to device\n    # This avoids the AttributeError: 'dict' object has no attribute 'word_ids'\n    word_ids = inputs_obj.word_ids(batch_index=0)\n    \n    # 3. Move to device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    inputs = {k: v.to(device) for k, v in inputs_obj.items()}\n    \n    # 4. Predict\n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    predictions = torch.argmax(outputs.logits, dim=-1)[0].cpu().numpy()\n    \n    # 5. Align predictions with original tokens\n    previous_word_idx = None\n    predictions_aligned = []\n    \n    for idx, word_idx in enumerate(word_ids):\n        # We only care about the first sub-token of each word\n        if word_idx is not None and word_idx != previous_word_idx:\n            predictions_aligned.append(predictions[idx])\n        previous_word_idx = word_idx\n    \n    # 6. Format output and extract entities\n    entities = []\n    current_entity = None\n    current_start = None\n    \n    # Iterate through tokens and their aligned predicted label indices\n    for i, (token, pred_idx) in enumerate(zip(tokens, predictions_aligned)):\n        label = config.LABEL_NAMES[pred_idx]\n        \n        # Start of a new entity\n        if label.startswith(\"B-\"):\n            # If we were already tracking an entity, save it before starting new one\n            if current_entity:\n                entities.append(f\"{current_entity}: {' '.join(tokens[current_start:i])}\")\n            current_entity = label[2:] # Strip the \"B-\"\n            current_start = i\n            \n        # Inside an entity (I-tag)\n        elif label.startswith(\"I-\"):\n            # If it's a continuation of the same type, we just keep going\n            # If we aren't tracking or type changed, treat it as a new entity (safety)\n            if not current_entity or current_entity != label[2:]:\n                if current_entity:\n                    entities.append(f\"{current_entity}: {' '.join(tokens[current_start:i])}\")\n                current_entity = label[2:]\n                current_start = i\n                \n        # Outside an entity (O-tag)\n        else:\n            if current_entity:\n                entities.append(f\"{current_entity}: {' '.join(tokens[current_start:i])}\")\n                current_entity = None\n                current_start = None\n    \n    # Add final entity if the sentence ended while tracking\n    if current_entity:\n        entities.append(f\"{current_entity}: {' '.join(tokens[current_start:])}\")\n    \n    print(f\"Entities found: {entities}\")\n    return entities\n\n# Test\ntest_texts = [\n    \"Apple was founded by Steve Jobs in Cupertino.\",\n    \"Berlin is the capital of Germany.\",\n    \"Google is a technology company based in Mountain View.\"\n]\n\n# Ensure student_model and tokenizer are loaded and ready\nfor text in test_texts:\n    test_distilled_inference(student_model, student_tokenizer, text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:39:23.587519Z","iopub.execute_input":"2026-01-07T16:39:23.587820Z","iopub.status.idle":"2026-01-07T16:39:23.654676Z","shell.execute_reply.started":"2026-01-07T16:39:23.587793Z","shell.execute_reply":"2026-01-07T16:39:23.654000Z"}},"outputs":[{"name":"stdout","text":"\nTesting inference with distilled model:\nText: Apple was founded by Steve Jobs in Cupertino.\nEntities found: ['ORG: Apple', 'PER: Steve Jobs', 'LOC: Cupertino.']\n\nTesting inference with distilled model:\nText: Berlin is the capital of Germany.\nEntities found: ['LOC: Berlin', 'LOC: Germany.']\n\nTesting inference with distilled model:\nText: Google is a technology company based in Mountain View.\nEntities found: ['ORG: Google', 'LOC: Mountain View.']\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"# Cell 21: Finish W&B run\nwandb.finish()\nprint(\"\\nKnowledge distillation completed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:39:36.048906Z","iopub.execute_input":"2026-01-07T16:39:36.049237Z","iopub.status.idle":"2026-01-07T16:39:36.069071Z","shell.execute_reply.started":"2026-01-07T16:39:36.049208Z","shell.execute_reply":"2026-01-07T16:39:36.068523Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compression_ratio</td><td>▁</td></tr><tr><td>eval/accuracy</td><td>▁▅▇███</td></tr><tr><td>eval/f1</td><td>▁▄▇███</td></tr><tr><td>eval/loss</td><td>█▅▂▁▁▁</td></tr><tr><td>eval/precision</td><td>▁▄████</td></tr><tr><td>eval/recall</td><td>▁▄▇███</td></tr><tr><td>eval/runtime</td><td>█▄▁▄▂▄</td></tr><tr><td>eval/samples_per_second</td><td>▁▅█▅▇▄</td></tr><tr><td>eval/steps_per_second</td><td>▁█████</td></tr><tr><td>eval_accuracy</td><td>▁▅▇███</td></tr><tr><td>+12</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compression_ratio</td><td>2.01416</td></tr><tr><td>eval/accuracy</td><td>0.93784</td></tr><tr><td>eval/f1</td><td>0.82896</td></tr><tr><td>eval/loss</td><td>6.18332</td></tr><tr><td>eval/precision</td><td>0.81893</td></tr><tr><td>eval/recall</td><td>0.83924</td></tr><tr><td>eval/runtime</td><td>78.4782</td></tr><tr><td>eval/samples_per_second</td><td>63.686</td></tr><tr><td>eval/steps_per_second</td><td>1.007</td></tr><tr><td>eval_accuracy</td><td>0.93784</td></tr><tr><td>+17</td><td>...</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"You can sync this run to the cloud by running:<br><code>wandb sync /kaggle/working/wandb/offline-run-20260107_153426-lh1y9gxk<code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/offline-run-20260107_153426-lh1y9gxk/logs</code>"},"metadata":{}},{"name":"stdout","text":"\nKnowledge distillation completed successfully!\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"!rm -rf /kaggle/working/models/student_distilled/checkpoint-2500","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:40:35.694839Z","iopub.execute_input":"2026-01-07T16:40:35.695228Z","iopub.status.idle":"2026-01-07T16:40:36.353367Z","shell.execute_reply.started":"2026-01-07T16:40:35.695199Z","shell.execute_reply":"2026-01-07T16:40:36.352437Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":67},{"cell_type":"markdown","source":"# notebooks/04_hyperparameter_tuning.ipynb","metadata":{}},{"cell_type":"code","source":"# Cell 1: Setup and imports\n!pip install optuna transformers[torch] datasets accelerate\n!pip install plotly kaleido  # For visualization\n\nimport optuna\nfrom optuna.trial import TrialState\nimport torch\nimport numpy as np\nfrom datasets import load_dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForTokenClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForTokenClassification\n)\nimport json\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom functools import partial\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Set seeds\ndef set_seed(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:48:45.766666Z","iopub.execute_input":"2026-01-07T16:48:45.767002Z","iopub.status.idle":"2026-01-07T16:48:56.078444Z","shell.execute_reply.started":"2026-01-07T16:48:45.766972Z","shell.execute_reply":"2026-01-07T16:48:56.077807Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (2.10.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.1)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\nRequirement already satisfied: transformers[torch] in /usr/local/lib/python3.12/dist-packages (4.57.1)\nRequirement already satisfied: alembic in /usr/local/lib/python3.12/dist-packages (from optuna) (1.4.3)\nRequirement already satisfied: cliff in /usr/local/lib/python3.12/dist-packages (from optuna) (4.13.0)\nRequirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (0.12.0)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna) (6.10.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\nRequirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.15.3)\nRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.2.19)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (3.20.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.36.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.32.5)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.6.2)\nRequirement already satisfied: torch>=2.2 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.8.0+cu126)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.11.12)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (1.2.1rc0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (2.6.2)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.4.0)\nRequirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic->optuna) (1.3.10)\nRequirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.12/dist-packages (from alembic->optuna) (1.0.4)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from alembic->optuna) (2.9.0.post0)\nRequirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from cliff->optuna) (0.5.2)\nRequirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from cliff->optuna) (2.7.0)\nRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from cliff->optuna) (3.16.0)\nRequirement already satisfied: stevedore>=5.6.0 in /usr/local/lib/python3.12/dist-packages (from cliff->optuna) (5.6.0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: pyperclip>=1.8 in /usr/local/lib/python3.12/dist-packages (from cmd2>=1.0.0->cliff->optuna) (1.11.0)\nRequirement already satisfied: rich-argparse>=1.7.1 in /usr/local/lib/python3.12/dist-packages (from cmd2>=1.0.0->cliff->optuna) (1.7.2)\nRequirement already satisfied: wcwidth>=0.2.10 in /usr/local/lib/python3.12/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.14)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil->alembic->optuna) (1.17.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.2->transformers[torch]) (3.0.3)\nRequirement already satisfied: rich>=11.0.0 in /usr/local/lib/python3.12/dist-packages (from rich-argparse>=1.7.1->cmd2>=1.0.0->cliff->optuna) (14.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.0.0->rich-argparse>=1.7.1->cmd2>=1.0.0->cliff->optuna) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.0.0->rich-argparse>=1.7.1->cmd2>=1.0.0->cliff->optuna) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.0.0->rich-argparse>=1.7.1->cmd2>=1.0.0->cliff->optuna) (0.1.2)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\nCollecting kaleido\n  Downloading kaleido-1.2.0-py3-none-any.whl.metadata (5.6 kB)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (8.5.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly) (25.0)\nCollecting choreographer>=1.1.1 (from kaleido)\n  Downloading choreographer-1.2.1-py3-none-any.whl.metadata (6.8 kB)\nCollecting logistro>=1.0.8 (from kaleido)\n  Downloading logistro-2.0.1-py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: orjson>=3.10.15 in /usr/local/lib/python3.12/dist-packages (from kaleido) (3.11.3)\nCollecting pytest-timeout>=2.4.0 (from kaleido)\n  Downloading pytest_timeout-2.4.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: simplejson>=3.19.3 in /usr/local/lib/python3.12/dist-packages (from choreographer>=1.1.1->kaleido) (3.20.2)\nRequirement already satisfied: pytest>=7.0.0 in /usr/local/lib/python3.12/dist-packages (from pytest-timeout>=2.4.0->kaleido) (8.4.2)\nRequirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (2.3.0)\nRequirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (1.6.0)\nRequirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (2.19.2)\nDownloading kaleido-1.2.0-py3-none-any.whl (68 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading choreographer-1.2.1-py3-none-any.whl (49 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading logistro-2.0.1-py3-none-any.whl (8.6 kB)\nDownloading pytest_timeout-2.4.0-py3-none-any.whl (14 kB)\nInstalling collected packages: logistro, pytest-timeout, choreographer, kaleido\nSuccessfully installed choreographer-1.2.1 kaleido-1.2.0 logistro-2.0.1 pytest-timeout-2.4.0\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/query.py:195: SyntaxWarning: \"is not\" with 'tuple' literal. Did you mean \"!=\"?\n  if entities is not ():\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"# Cell 2: Configuration\nclass OptunaConfig:\n    STUDENT_MODEL = \"xlm-roberta-base\"\n    TEACHER_MODEL = \"./models/teacher\"  # For distillation\n    DATASET_NAME = \"wikiann\"\n    LANGUAGES = [\"en\", \"de\"]  # Use subset for faster tuning\n    MAX_LENGTH = 128\n    NUM_LABELS = 7\n    OUTPUT_DIR = \"./models/optuna_tuned\"\n    N_TRIALS = 10  # Number of Optuna trials\n    TIMEOUT = 3600  # 1 hour timeout\n    LABEL_NAMES = [\n        \"O\",\n        \"B-PER\", \"I-PER\",\n        \"B-ORG\", \"I-ORG\",\n        \"B-LOC\", \"I-LOC\"\n    ]\n\nconfig = OptunaConfig()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T17:42:49.737879Z","iopub.execute_input":"2026-01-07T17:42:49.738719Z","iopub.status.idle":"2026-01-07T17:42:49.743627Z","shell.execute_reply.started":"2026-01-07T17:42:49.738687Z","shell.execute_reply":"2026-01-07T17:42:49.742973Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"# Cell 3: Load and prepare dataset\ndef prepare_dataset():\n    datasets = {}\n    \n    for lang in config.LANGUAGES:\n        try:\n            print(f\"Loading {lang} dataset...\")\n            dataset = load_dataset(config.DATASET_NAME, lang)\n            datasets[lang] = dataset\n        except Exception as e:\n            print(f\"Error loading {lang}: {e}\")\n    \n    # Combine and split for tuning (small subset)\n    combined_train = []\n    combined_val = []\n    \n    for lang, dataset in datasets.items():\n        # Take smaller subset for faster tuning\n        train_subset = dataset['train'].select(range(2000))\n        val_subset = dataset['validation'].select(range(500))\n        \n        combined_train.extend(train_subset)\n        combined_val.extend(val_subset)\n    \n    return DatasetDict({\n        'train': combined_train,\n        'validation': combined_val\n    })\n\ndataset = prepare_dataset()\nprint(f\"Training samples: {len(dataset['train'])}\")\nprint(f\"Validation samples: {len(dataset['validation'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:49:09.767313Z","iopub.execute_input":"2026-01-07T16:49:09.767596Z","iopub.status.idle":"2026-01-07T16:49:13.759893Z","shell.execute_reply.started":"2026-01-07T16:49:09.767575Z","shell.execute_reply":"2026-01-07T16:49:13.759118Z"}},"outputs":[{"name":"stdout","text":"Loading en dataset...\nLoading de dataset...\nTraining samples: 4000\nValidation samples: 1000\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\n\n# --- RE-CONVERSION STEP ---\n# This converts your lists back into Hugging Face Dataset objects\nif isinstance(dataset[\"train\"], list):\n    print(\"Detected list format. Re-converting to Hugging Face Datasets...\")\n    dataset = DatasetDict({\n        split: Dataset.from_list(dataset[split]) \n        for split in dataset.keys()\n    })\n\n# Now the rest of your code will work perfectly\ntokenizer = AutoTokenizer.from_pretrained(config.STUDENT_MODEL)\n\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"],\n        truncation=True,\n        is_split_into_words=True,\n        max_length=config.MAX_LENGTH,\n        padding=\"max_length\"\n    )\n    \n    labels = []\n    for i, label in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        \n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            else:\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    \n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n\n# Identify columns to remove (now .column_names will work!)\ncolumns_to_remove = dataset[\"train\"].column_names\n\n# Map the function\ntokenized_dataset = dataset.map(\n    tokenize_and_align_labels,\n    batched=True,\n    remove_columns=columns_to_remove\n)\n\nprint(\"\\nSuccess! Dataset tokenized and aligned.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:54:00.318267Z","iopub.execute_input":"2026-01-07T16:54:00.318766Z","iopub.status.idle":"2026-01-07T16:54:02.700813Z","shell.execute_reply.started":"2026-01-07T16:54:00.318733Z","shell.execute_reply":"2026-01-07T16:54:02.700233Z"}},"outputs":[{"name":"stdout","text":"Detected list format. Re-converting to Hugging Face Datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b7863d17b2e4eaf93b1aeab431a1a5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a88d157593454e34a94b5e89198e0ccf"}},"metadata":{}},{"name":"stdout","text":"\nSuccess! Dataset tokenized and aligned.\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"# Cell 5: Define objective function for Optuna\nimport torch\nimport gc\nimport os\nimport shutil\nimport wandb\nfrom transformers import AutoModelForTokenClassification, TrainingArguments, DataCollatorForTokenClassification\n\ndef objective(trial, tokenized_dataset):\n    # 1. Initialize wandb in OFFLINE mode for this specific trial\n    # This prevents the \"must call wandb.init()\" error by giving the trainer a local run\n    run = wandb.init(\n        project=\"ner_distillation_optuna\",\n        name=f\"trial_{trial.number}\",\n        mode=\"offline\",\n        reinit=True,\n        anonymous=\"allow\"\n    )\n\n    # Suggest hyperparameters\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 2, 4) # Reduced for tuning speed\n    \n    # Distillation specific parameters\n    temperature = trial.suggest_float(\"temperature\", 1.0, 5.0)\n    alpha = trial.suggest_float(\"alpha\", 0.1, 0.9)\n    \n    # Initialize Student Model\n    # FIX: Added explicit id2label/label2id mapping to ensure F1 isn't 0.0\n    model = AutoModelForTokenClassification.from_pretrained(\n        config.STUDENT_MODEL,\n        num_labels=config.NUM_LABELS,\n        id2label={i: label for i, label in enumerate(config.LABEL_NAMES)},\n        label2id={label: i for i, label in enumerate(config.LABEL_NAMES)}\n    )\n    \n    # Training arguments\n    trial_tmp_dir = f\"./tmp_trial_{trial.number}\"\n    training_args = TrainingArguments(\n        output_dir=trial_tmp_dir,\n        eval_strategy=\"epoch\",\n        learning_rate=learning_rate,\n        per_device_train_batch_size=batch_size,\n        num_train_epochs=num_train_epochs,\n        logging_steps=50,\n        report_to=\"none\",          # Disable automatic cloud logging\n        skip_memory_metrics=True,   # Helps prevent extra logging calls\n        save_strategy=\"no\",         # FIX: Don't save weights to save disk space\n        fp16=torch.cuda.is_available(),\n        # This is required for Optuna pruning to work\n        disable_tqdm=True \n    )\n    \n    # USE YOUR DistillationTrainer HERE\n    trainer = DistillationTrainer(\n        model=model,\n        teacher_model=teacher_model, # Ensure teacher_model is global or passed in\n        temperature=temperature,\n        alpha=alpha,\n        args=training_args,\n        train_dataset=tokenized_dataset[\"train\"],\n        eval_dataset=tokenized_dataset[\"validation\"],\n        tokenizer=tokenizer,\n        data_collator=DataCollatorForTokenClassification(tokenizer),\n        compute_metrics=compute_metrics, # Use the function you defined\n    )\n    \n    try:\n        trainer.train()\n        eval_results = trainer.evaluate()\n        \n        # FIX: Ensure we use .get() to handle potential missing keys safely\n        score = eval_results.get(\"eval_f1\", 0.0)\n        \n        # CLEANUP: Finish wandb run\n        run.finish()\n        \n        # CLEANUP VRAM (Crucial for Optuna)\n        del model\n        del trainer\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        # CLEANUP DISK: Delete trial folder and local wandb logs to prevent 20GB limit crash\n        shutil.rmtree(trial_tmp_dir, ignore_errors=True)\n        if os.path.exists(\"./wandb\"):\n            shutil.rmtree(\"./wandb\", ignore_errors=True)\n        \n        return score\n        \n    except Exception as e:\n        # Cleanup even if it fails\n        if 'run' in locals():\n            run.finish(exit_code=1)\n        del model\n        gc.collect()\n        torch.cuda.empty_cache()\n        shutil.rmtree(trial_tmp_dir, ignore_errors=True)\n        print(f\"Trial {trial.number} failed: {e}\")\n        return 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T18:18:09.267639Z","iopub.execute_input":"2026-01-07T18:18:09.267954Z","iopub.status.idle":"2026-01-07T18:18:09.279250Z","shell.execute_reply.started":"2026-01-07T18:18:09.267928Z","shell.execute_reply":"2026-01-07T18:18:09.278303Z"}},"outputs":[],"execution_count":95},{"cell_type":"code","source":"# Cell 6: Create Optuna study\nimport os\nimport optuna\n\n# 1. Ensure the output directory exists\nos.makedirs(config.OUTPUT_DIR, exist_ok=True)\n\n# 2. Define the storage URL (using absolute path is safer in Kaggle)\nstorage_path = f\"sqlite:///{os.path.abspath(config.OUTPUT_DIR)}/optuna.db\"\nprint(f\"Database will be saved at: {storage_path}\")\n\n# 3. Create the study\ndef create_study():\n    study = optuna.create_study(\n        direction=\"maximize\",\n        study_name=\"multilingual_ner_distillation\",\n        storage=storage_path,\n        load_if_exists=True,\n        pruner=optuna.pruners.MedianPruner(\n            n_startup_trials=5,\n            n_warmup_steps=10,\n            interval_steps=1\n        )\n    )\n    return study\n\nstudy = create_study()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T18:18:12.075660Z","iopub.execute_input":"2026-01-07T18:18:12.076398Z","iopub.status.idle":"2026-01-07T18:18:12.123587Z","shell.execute_reply.started":"2026-01-07T18:18:12.076370Z","shell.execute_reply":"2026-01-07T18:18:12.122864Z"}},"outputs":[{"name":"stderr","text":"\u001b[32m[I 2026-01-07 18:18:12,113]\u001b[0m Using an existing study with name 'multilingual_ner_distillation' instead of creating a new one.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Database will be saved at: sqlite:////kaggle/working/models/optuna_tuned/optuna.db\n","output_type":"stream"}],"execution_count":96},{"cell_type":"code","source":"# Cell 7: Run optimization\nprint(f\"Starting Optuna optimization with {config.N_TRIALS} trials...\")\n\n# Create partial function for objective\nobjective_partial = partial(objective, tokenized_dataset=tokenized_dataset)\n\nstudy.optimize(\n    objective_partial,\n    n_trials=config.N_TRIALS,\n    timeout=config.TIMEOUT,\n    show_progress_bar=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T18:25:24.453038Z","iopub.execute_input":"2026-01-07T18:25:24.453882Z","iopub.status.idle":"2026-01-07T19:24:45.987816Z","shell.execute_reply.started":"2026-01-07T18:25:24.453849Z","shell.execute_reply":"2026-01-07T19:24:45.987010Z"}},"outputs":[{"name":"stdout","text":"Starting Optuna optimization with 10 trials...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7076937b44a422ea97fbf433172d979"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing previous runs because reinit is set to True."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>▁</td></tr><tr><td>eval_f1</td><td>▁</td></tr><tr><td>eval_precision</td><td>▁</td></tr><tr><td>eval_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>0.73147</td></tr><tr><td>eval_f1</td><td>0.32751</td></tr><tr><td>eval_precision</td><td>0.3146</td></tr><tr><td>eval_recall</td><td>0.34152</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"You can sync this run to the cloud by running:<br><code>wandb sync /kaggle/working/wandb/offline-run-20260107_182007-wvj3ld9v<code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/offline-run-20260107_182007-wvj3ld9v/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.22.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/kaggle/working/wandb/offline-run-20260107_182524-p8r1r74f</code>"},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 40.9999, 'grad_norm': 1243.584228515625, 'learning_rate': 1.0885374913252196e-05, 'epoch': 0.4}\n{'loss': 21.8792, 'grad_norm': 235.21066284179688, 'learning_rate': 9.215838883612288e-06, 'epoch': 0.8}\n{'eval_loss': 14.627320289611816, 'eval_precision': 0.21875, 'eval_recall': 0.046875, 'eval_f1': 0.07720588235294118, 'eval_accuracy': 0.6621171770972037, 'eval_runtime': 17.0032, 'eval_samples_per_second': 58.813, 'eval_steps_per_second': 7.352, 'epoch': 1.0}\n{'loss': 16.5289, 'grad_norm': 187.8052520751953, 'learning_rate': 7.546302853972381e-06, 'epoch': 1.2}\n{'loss': 12.6802, 'grad_norm': 409.6534423828125, 'learning_rate': 5.876766824332473e-06, 'epoch': 1.6}\n{'loss': 11.1215, 'grad_norm': 437.83831787109375, 'learning_rate': 4.207230794692566e-06, 'epoch': 2.0}\n{'eval_loss': 8.585430145263672, 'eval_precision': 0.5945945945945946, 'eval_recall': 0.5892857142857143, 'eval_f1': 0.5919282511210763, 'eval_accuracy': 0.7795162006213937, 'eval_runtime': 17.0069, 'eval_samples_per_second': 58.8, 'eval_steps_per_second': 7.35, 'epoch': 2.0}\n{'loss': 9.6572, 'grad_norm': 143.2374267578125, 'learning_rate': 2.537694765052659e-06, 'epoch': 2.4}\n{'loss': 9.159, 'grad_norm': 134.49586486816406, 'learning_rate': 8.681587354127518e-07, 'epoch': 2.8}\n{'eval_loss': 7.714200019836426, 'eval_precision': 0.6323639075316928, 'eval_recall': 0.6309523809523809, 'eval_f1': 0.631657355679702, 'eval_accuracy': 0.7866178428761651, 'eval_runtime': 16.9986, 'eval_samples_per_second': 58.828, 'eval_steps_per_second': 7.354, 'epoch': 3.0}\n{'train_runtime': 330.9509, 'train_samples_per_second': 36.259, 'train_steps_per_second': 1.133, 'train_loss': 16.930508138020834, 'epoch': 3.0}\n{'eval_loss': 7.714200019836426, 'eval_precision': 0.6323639075316928, 'eval_recall': 0.6309523809523809, 'eval_f1': 0.631657355679702, 'eval_accuracy': 0.7866178428761651, 'eval_runtime': 16.9816, 'eval_samples_per_second': 58.887, 'eval_steps_per_second': 7.361, 'epoch': 3.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>▁███</td></tr><tr><td>eval_f1</td><td>▁▇██</td></tr><tr><td>eval_precision</td><td>▁▇██</td></tr><tr><td>eval_recall</td><td>▁███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>0.78662</td></tr><tr><td>eval_f1</td><td>0.63166</td></tr><tr><td>eval_precision</td><td>0.63236</td></tr><tr><td>eval_recall</td><td>0.63095</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"You can sync this run to the cloud by running:<br><code>wandb sync /kaggle/working/wandb/offline-run-20260107_182524-p8r1r74f<code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/offline-run-20260107_182524-p8r1r74f/logs</code>"},"metadata":{}},{"name":"stdout","text":"\u001b[32m[I 2026-01-07 18:31:21,260]\u001b[0m Trial 26 finished with value: 0.631657355679702 and parameters: {'learning_rate': 1.2521520222299304e-05, 'batch_size': 32, 'num_train_epochs': 3, 'temperature': 1.7608725922213273, 'alpha': 0.20448198842908563}. Best is trial 26 with value: 0.631657355679702.\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.22.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/kaggle/working/wandb/offline-run-20260107_183121-h1d4fdqb</code>"},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 26.4911, 'grad_norm': 141.0808868408203, 'learning_rate': 2.0422284882461512e-05, 'epoch': 0.4}\n{'loss': 10.4027, 'grad_norm': 205.85720825195312, 'learning_rate': 1.729003259987539e-05, 'epoch': 0.8}\n{'eval_loss': 5.9945573806762695, 'eval_precision': 0.4955264969029594, 'eval_recall': 0.5357142857142857, 'eval_f1': 0.5148373257061137, 'eval_accuracy': 0.7729693741677763, 'eval_runtime': 17.0141, 'eval_samples_per_second': 58.775, 'eval_steps_per_second': 7.347, 'epoch': 1.0}\n{'loss': 6.7275, 'grad_norm': 144.6412353515625, 'learning_rate': 1.4157780317289271e-05, 'epoch': 1.2}\n{'loss': 5.4868, 'grad_norm': 114.36744689941406, 'learning_rate': 1.1025528034703148e-05, 'epoch': 1.6}\n{'loss': 5.013, 'grad_norm': 535.359619140625, 'learning_rate': 7.893275752117028e-06, 'epoch': 2.0}\n{'eval_loss': 3.8753435611724854, 'eval_precision': 0.6562731997030438, 'eval_recall': 0.6577380952380952, 'eval_f1': 0.6570048309178744, 'eval_accuracy': 0.8196848646249445, 'eval_runtime': 16.9959, 'eval_samples_per_second': 58.838, 'eval_steps_per_second': 7.355, 'epoch': 2.0}\n{'loss': 3.9163, 'grad_norm': 124.83087921142578, 'learning_rate': 4.761023469530905e-06, 'epoch': 2.4}\n{'loss': 3.8463, 'grad_norm': 100.9517822265625, 'learning_rate': 1.6287711869447834e-06, 'epoch': 2.8}\n{'eval_loss': 3.552267074584961, 'eval_precision': 0.6890881913303438, 'eval_recall': 0.6860119047619048, 'eval_f1': 0.6875466070096943, 'eval_accuracy': 0.8591877496671105, 'eval_runtime': 17.0189, 'eval_samples_per_second': 58.758, 'eval_steps_per_second': 7.345, 'epoch': 3.0}\n{'train_runtime': 330.6814, 'train_samples_per_second': 36.289, 'train_steps_per_second': 1.134, 'train_loss': 8.510489929199219, 'epoch': 3.0}\n{'eval_loss': 3.552267074584961, 'eval_precision': 0.6890881913303438, 'eval_recall': 0.6860119047619048, 'eval_f1': 0.6875466070096943, 'eval_accuracy': 0.8591877496671105, 'eval_runtime': 16.999, 'eval_samples_per_second': 58.827, 'eval_steps_per_second': 7.353, 'epoch': 3.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>▁▅██</td></tr><tr><td>eval_f1</td><td>▁▇██</td></tr><tr><td>eval_precision</td><td>▁▇██</td></tr><tr><td>eval_recall</td><td>▁▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>0.85919</td></tr><tr><td>eval_f1</td><td>0.68755</td></tr><tr><td>eval_precision</td><td>0.68909</td></tr><tr><td>eval_recall</td><td>0.68601</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"You can sync this run to the cloud by running:<br><code>wandb sync /kaggle/working/wandb/offline-run-20260107_183121-h1d4fdqb<code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/offline-run-20260107_183121-h1d4fdqb/logs</code>"},"metadata":{}},{"name":"stdout","text":"\u001b[32m[I 2026-01-07 18:37:17,518]\u001b[0m Trial 27 finished with value: 0.6875466070096943 and parameters: {'learning_rate': 2.3491892119395914e-05, 'batch_size': 32, 'num_train_epochs': 3, 'temperature': 1.1300696007500195, 'alpha': 0.19537325061160385}. Best is trial 27 with value: 0.6875466070096943.\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.22.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/kaggle/working/wandb/offline-run-20260107_183717-vjk82njs</code>"},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 48.3234, 'grad_norm': 197.7684326171875, 'learning_rate': 1.0717442627396396e-05, 'epoch': 0.4}\n{'loss': 24.8315, 'grad_norm': 291.3287353515625, 'learning_rate': 9.073663083317194e-06, 'epoch': 0.8}\n{'eval_loss': 17.048328399658203, 'eval_precision': 0.20229007633587787, 'eval_recall': 0.03943452380952381, 'eval_f1': 0.0660024906600249, 'eval_accuracy': 0.6532401242787395, 'eval_runtime': 16.9625, 'eval_samples_per_second': 58.954, 'eval_steps_per_second': 7.369, 'epoch': 1.0}\n{'loss': 18.7788, 'grad_norm': 187.36630249023438, 'learning_rate': 7.429883539237994e-06, 'epoch': 1.2}\n{'loss': 13.9335, 'grad_norm': 226.0090789794922, 'learning_rate': 5.786103995158791e-06, 'epoch': 1.6}\n{'loss': 12.0049, 'grad_norm': 358.4893798828125, 'learning_rate': 4.142324451079589e-06, 'epoch': 2.0}\n{'eval_loss': 9.516107559204102, 'eval_precision': 0.5289139633286318, 'eval_recall': 0.5580357142857143, 'eval_f1': 0.5430847212165099, 'eval_accuracy': 0.7789613848202397, 'eval_runtime': 16.9714, 'eval_samples_per_second': 58.923, 'eval_steps_per_second': 7.365, 'epoch': 2.0}\n{'loss': 10.4519, 'grad_norm': 145.1882781982422, 'learning_rate': 2.498544907000387e-06, 'epoch': 2.4}\n{'loss': 9.8066, 'grad_norm': 204.59634399414062, 'learning_rate': 8.54765362921185e-07, 'epoch': 2.8}\n{'eval_loss': 8.534073829650879, 'eval_precision': 0.5899122807017544, 'eval_recall': 0.6004464285714286, 'eval_f1': 0.5951327433628318, 'eval_accuracy': 0.7858411007545495, 'eval_runtime': 16.9844, 'eval_samples_per_second': 58.877, 'eval_steps_per_second': 7.36, 'epoch': 3.0}\n{'train_runtime': 331.1483, 'train_samples_per_second': 36.238, 'train_steps_per_second': 1.132, 'train_loss': 19.12015275065104, 'epoch': 3.0}\n{'eval_loss': 8.534073829650879, 'eval_precision': 0.5899122807017544, 'eval_recall': 0.6004464285714286, 'eval_f1': 0.5951327433628318, 'eval_accuracy': 0.7858411007545495, 'eval_runtime': 16.964, 'eval_samples_per_second': 58.948, 'eval_steps_per_second': 7.369, 'epoch': 3.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>▁███</td></tr><tr><td>eval_f1</td><td>▁▇██</td></tr><tr><td>eval_precision</td><td>▁▇██</td></tr><tr><td>eval_recall</td><td>▁▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>0.78584</td></tr><tr><td>eval_f1</td><td>0.59513</td></tr><tr><td>eval_precision</td><td>0.58991</td></tr><tr><td>eval_recall</td><td>0.60045</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"You can sync this run to the cloud by running:<br><code>wandb sync /kaggle/working/wandb/offline-run-20260107_183717-vjk82njs<code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/offline-run-20260107_183717-vjk82njs/logs</code>"},"metadata":{}},{"name":"stdout","text":"\u001b[32m[I 2026-01-07 18:43:14,066]\u001b[0m Trial 28 finished with value: 0.5951327433628318 and parameters: {'learning_rate': 1.2328346580594015e-05, 'batch_size': 32, 'num_train_epochs': 3, 'temperature': 2.2784146544121437, 'alpha': 0.18896629451692906}. Best is trial 27 with value: 0.6875466070096943.\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.22.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/kaggle/working/wandb/offline-run-20260107_184314-walq3cor</code>"},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 30.6083, 'grad_norm': 481.4521789550781, 'learning_rate': 2.1132666883530803e-05, 'epoch': 0.4}\n{'loss': 12.7802, 'grad_norm': 194.8902587890625, 'learning_rate': 1.789146030630215e-05, 'epoch': 0.8}\n{'eval_loss': 7.756994724273682, 'eval_precision': 0.4694996572995202, 'eval_recall': 0.5096726190476191, 'eval_f1': 0.4887620406707099, 'eval_accuracy': 0.7688637372392366, 'eval_runtime': 16.9835, 'eval_samples_per_second': 58.881, 'eval_steps_per_second': 7.36, 'epoch': 1.0}\n{'loss': 8.4104, 'grad_norm': 206.4488983154297, 'learning_rate': 1.4650253729073502e-05, 'epoch': 1.2}\n{'loss': 7.1932, 'grad_norm': 199.0181884765625, 'learning_rate': 1.1409047151844851e-05, 'epoch': 1.6}\n{'loss': 6.3554, 'grad_norm': 299.0316162109375, 'learning_rate': 8.1678405746162e-06, 'epoch': 2.0}\n{'eval_loss': 5.1164326667785645, 'eval_precision': 0.6605705925384052, 'eval_recall': 0.671875, 'eval_f1': 0.6661748432312798, 'eval_accuracy': 0.8020417221482468, 'eval_runtime': 16.9501, 'eval_samples_per_second': 58.997, 'eval_steps_per_second': 7.375, 'epoch': 2.0}\n{'loss': 4.9305, 'grad_norm': 87.93724060058594, 'learning_rate': 4.926633997387549e-06, 'epoch': 2.4}\n{'loss': 4.7744, 'grad_norm': nan, 'learning_rate': 1.6854274201588982e-06, 'epoch': 2.8}\n{'eval_loss': 4.232302665710449, 'eval_precision': 0.696969696969697, 'eval_recall': 0.6845238095238095, 'eval_f1': 0.6906906906906907, 'eval_accuracy': 0.8618508655126498, 'eval_runtime': 16.9738, 'eval_samples_per_second': 58.914, 'eval_steps_per_second': 7.364, 'epoch': 3.0}\n{'train_runtime': 330.4898, 'train_samples_per_second': 36.31, 'train_steps_per_second': 1.135, 'train_loss': 10.331460205078125, 'epoch': 3.0}\n{'eval_loss': 4.232302665710449, 'eval_precision': 0.696969696969697, 'eval_recall': 0.6845238095238095, 'eval_f1': 0.6906906906906907, 'eval_accuracy': 0.8618508655126498, 'eval_runtime': 16.9639, 'eval_samples_per_second': 58.949, 'eval_steps_per_second': 7.369, 'epoch': 3.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>▁▃██</td></tr><tr><td>eval_f1</td><td>▁▇██</td></tr><tr><td>eval_precision</td><td>▁▇██</td></tr><tr><td>eval_recall</td><td>▁▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>0.86185</td></tr><tr><td>eval_f1</td><td>0.69069</td></tr><tr><td>eval_precision</td><td>0.69697</td></tr><tr><td>eval_recall</td><td>0.68452</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"You can sync this run to the cloud by running:<br><code>wandb sync /kaggle/working/wandb/offline-run-20260107_184314-walq3cor<code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/offline-run-20260107_184314-walq3cor/logs</code>"},"metadata":{}},{"name":"stdout","text":"\u001b[32m[I 2026-01-07 18:49:10,020]\u001b[0m Trial 29 finished with value: 0.6906906906906907 and parameters: {'learning_rate': 2.430904932921488e-05, 'batch_size': 32, 'num_train_epochs': 3, 'temperature': 1.489960090219177, 'alpha': 0.1769077338286403}. Best is trial 29 with value: 0.6906906906906907.\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.22.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/kaggle/working/wandb/offline-run-20260107_184910-j9rpunx4</code>"},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 26.8862, 'grad_norm': 119.54466247558594, 'learning_rate': 2.1179435032548774e-05, 'epoch': 0.4}\n{'loss': 10.6632, 'grad_norm': 188.08860778808594, 'learning_rate': 1.7931055426329635e-05, 'epoch': 0.8}\n{'eval_loss': 6.322642803192139, 'eval_precision': 0.5195493704440026, 'eval_recall': 0.5833333333333334, 'eval_f1': 0.5495969155275149, 'eval_accuracy': 0.7776298268974701, 'eval_runtime': 16.9773, 'eval_samples_per_second': 58.902, 'eval_steps_per_second': 7.363, 'epoch': 1.0}\n{'loss': 7.212, 'grad_norm': 117.23246002197266, 'learning_rate': 1.46826758201105e-05, 'epoch': 1.2}\n{'loss': 5.7752, 'grad_norm': 198.23023986816406, 'learning_rate': 1.1434296213891363e-05, 'epoch': 1.6}\n{'loss': 5.2527, 'grad_norm': 207.98794555664062, 'learning_rate': 8.185916607672225e-06, 'epoch': 2.0}\n{'eval_loss': 4.326265811920166, 'eval_precision': 0.6561151079136691, 'eval_recall': 0.6785714285714286, 'eval_f1': 0.6671543525969276, 'eval_accuracy': 0.8511984021304927, 'eval_runtime': 16.9509, 'eval_samples_per_second': 58.994, 'eval_steps_per_second': 7.374, 'epoch': 2.0}\n{'loss': 4.2091, 'grad_norm': 85.60433959960938, 'learning_rate': 4.9375370014530885e-06, 'epoch': 2.4}\n{'loss': 4.0476, 'grad_norm': 110.10029602050781, 'learning_rate': 1.6891573952339511e-06, 'epoch': 2.8}\n{'eval_loss': 3.7649292945861816, 'eval_precision': 0.6906158357771262, 'eval_recall': 0.7008928571428571, 'eval_f1': 0.6957163958641064, 'eval_accuracy': 0.8710608078118065, 'eval_runtime': 16.9681, 'eval_samples_per_second': 58.934, 'eval_steps_per_second': 7.367, 'epoch': 3.0}\n{'train_runtime': 330.4572, 'train_samples_per_second': 36.313, 'train_steps_per_second': 1.135, 'train_loss': 8.807680786132812, 'epoch': 3.0}\n{'eval_loss': 3.7649292945861816, 'eval_precision': 0.6906158357771262, 'eval_recall': 0.7008928571428571, 'eval_f1': 0.6957163958641064, 'eval_accuracy': 0.8710608078118065, 'eval_runtime': 16.9636, 'eval_samples_per_second': 58.95, 'eval_steps_per_second': 7.369, 'epoch': 3.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>▁▇██</td></tr><tr><td>eval_f1</td><td>▁▇██</td></tr><tr><td>eval_precision</td><td>▁▇██</td></tr><tr><td>eval_recall</td><td>▁▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>0.87106</td></tr><tr><td>eval_f1</td><td>0.69572</td></tr><tr><td>eval_precision</td><td>0.69062</td></tr><tr><td>eval_recall</td><td>0.70089</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"You can sync this run to the cloud by running:<br><code>wandb sync /kaggle/working/wandb/offline-run-20260107_184910-j9rpunx4<code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/offline-run-20260107_184910-j9rpunx4/logs</code>"},"metadata":{}},{"name":"stdout","text":"\u001b[32m[I 2026-01-07 18:55:05,953]\u001b[0m Trial 30 finished with value: 0.6957163958641064 and parameters: {'learning_rate': 2.4362847046643528e-05, 'batch_size': 32, 'num_train_epochs': 3, 'temperature': 1.5167020975930399, 'alpha': 0.15446500498432433}. Best is trial 30 with value: 0.6957163958641064.\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.22.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/kaggle/working/wandb/offline-run-20260107_185505-nsrb8rjq</code>"},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 20.4513, 'grad_norm': 168.60934448242188, 'learning_rate': 2.1627685982781038e-05, 'epoch': 0.4}\n{'loss': 8.3113, 'grad_norm': 190.53866577148438, 'learning_rate': 1.8310556230820757e-05, 'epoch': 0.8}\n{'eval_loss': 4.995506763458252, 'eval_precision': 0.43550531914893614, 'eval_recall': 0.48735119047619047, 'eval_f1': 0.4599719101123595, 'eval_accuracy': 0.7671992898357746, 'eval_runtime': 16.9797, 'eval_samples_per_second': 58.894, 'eval_steps_per_second': 7.362, 'epoch': 1.0}\n{'loss': 5.455, 'grad_norm': 107.02422332763672, 'learning_rate': 1.4993426478860475e-05, 'epoch': 1.2}\n{'loss': 4.3262, 'grad_norm': 109.27323150634766, 'learning_rate': 1.1676296726900192e-05, 'epoch': 1.6}\n{'loss': 3.9067, 'grad_norm': 256.5289001464844, 'learning_rate': 8.359166974939911e-06, 'epoch': 2.0}\n{'eval_loss': 3.3655638694763184, 'eval_precision': 0.6199859254046446, 'eval_recall': 0.6555059523809523, 'eval_f1': 0.637251356238698, 'eval_accuracy': 0.8500887705281847, 'eval_runtime': 16.9826, 'eval_samples_per_second': 58.884, 'eval_steps_per_second': 7.36, 'epoch': 2.0}\n{'loss': 3.0789, 'grad_norm': 91.12408447265625, 'learning_rate': 5.042037222979628e-06, 'epoch': 2.4}\n{'loss': 2.973, 'grad_norm': 76.8056411743164, 'learning_rate': 1.7249074710193465e-06, 'epoch': 2.8}\n{'eval_loss': 2.8722567558288574, 'eval_precision': 0.6794117647058824, 'eval_recall': 0.6875, 'eval_f1': 0.6834319526627219, 'eval_accuracy': 0.8790501553484243, 'eval_runtime': 17.0064, 'eval_samples_per_second': 58.801, 'eval_steps_per_second': 7.35, 'epoch': 3.0}\n{'train_runtime': 330.5397, 'train_samples_per_second': 36.304, 'train_steps_per_second': 1.135, 'train_loss': 6.663476908365886, 'epoch': 3.0}\n{'eval_loss': 2.8722567558288574, 'eval_precision': 0.6794117647058824, 'eval_recall': 0.6875, 'eval_f1': 0.6834319526627219, 'eval_accuracy': 0.8790501553484243, 'eval_runtime': 16.9667, 'eval_samples_per_second': 58.939, 'eval_steps_per_second': 7.367, 'epoch': 3.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>▁▆██</td></tr><tr><td>eval_f1</td><td>▁▇██</td></tr><tr><td>eval_precision</td><td>▁▆██</td></tr><tr><td>eval_recall</td><td>▁▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>0.87905</td></tr><tr><td>eval_f1</td><td>0.68343</td></tr><tr><td>eval_precision</td><td>0.67941</td></tr><tr><td>eval_recall</td><td>0.6875</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"You can sync this run to the cloud by running:<br><code>wandb sync /kaggle/working/wandb/offline-run-20260107_185505-nsrb8rjq<code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/offline-run-20260107_185505-nsrb8rjq/logs</code>"},"metadata":{}},{"name":"stdout","text":"\u001b[32m[I 2026-01-07 19:01:02,049]\u001b[0m Trial 31 finished with value: 0.6834319526627219 and parameters: {'learning_rate': 2.4878473139702115e-05, 'batch_size': 32, 'num_train_epochs': 3, 'temperature': 1.0454373763130327, 'alpha': 0.16997711293111242}. Best is trial 30 with value: 0.6957163958641064.\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.22.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/kaggle/working/wandb/offline-run-20260107_190102-ejs587jx</code>"},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 17.2989, 'grad_norm': 74.47254943847656, 'learning_rate': 2.394529183368379e-05, 'epoch': 0.4}\n{'loss': 7.8294, 'grad_norm': 129.23287963867188, 'learning_rate': 2.02727010616464e-05, 'epoch': 0.8}\n{'eval_loss': 4.480818271636963, 'eval_precision': 0.4455891822279459, 'eval_recall': 0.5148809523809523, 'eval_f1': 0.4777355885398688, 'eval_accuracy': 0.7650909897913892, 'eval_runtime': 16.9836, 'eval_samples_per_second': 58.88, 'eval_steps_per_second': 7.36, 'epoch': 1.0}\n{'loss': 5.1105, 'grad_norm': 109.77853393554688, 'learning_rate': 1.660011028960901e-05, 'epoch': 1.2}\n{'loss': 4.0142, 'grad_norm': 87.73076629638672, 'learning_rate': 1.2927519517571617e-05, 'epoch': 1.6}\n{'loss': 3.7465, 'grad_norm': 213.06777954101562, 'learning_rate': 9.254928745534226e-06, 'epoch': 2.0}\n{'eval_loss': 2.916012763977051, 'eval_precision': 0.64375, 'eval_recall': 0.6897321428571429, 'eval_f1': 0.665948275862069, 'eval_accuracy': 0.8634043497558811, 'eval_runtime': 16.9625, 'eval_samples_per_second': 58.953, 'eval_steps_per_second': 7.369, 'epoch': 2.0}\n{'loss': 2.8914, 'grad_norm': 54.09122085571289, 'learning_rate': 5.582337973496834e-06, 'epoch': 2.4}\n{'loss': 2.7462, 'grad_norm': 74.46435546875, 'learning_rate': 1.9097472014594435e-06, 'epoch': 2.8}\n{'eval_loss': 2.433969736099243, 'eval_precision': 0.7019579405366208, 'eval_recall': 0.7202380952380952, 'eval_f1': 0.7109805361733381, 'eval_accuracy': 0.8932534398579671, 'eval_runtime': 16.9778, 'eval_samples_per_second': 58.9, 'eval_steps_per_second': 7.363, 'epoch': 3.0}\n{'train_runtime': 330.4746, 'train_samples_per_second': 36.311, 'train_steps_per_second': 1.135, 'train_loss': 6.000695170084636, 'epoch': 3.0}\n{'eval_loss': 2.433969736099243, 'eval_precision': 0.7019579405366208, 'eval_recall': 0.7202380952380952, 'eval_f1': 0.7109805361733381, 'eval_accuracy': 0.8932534398579671, 'eval_runtime': 16.9802, 'eval_samples_per_second': 58.892, 'eval_steps_per_second': 7.362, 'epoch': 3.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>▁▆██</td></tr><tr><td>eval_f1</td><td>▁▇██</td></tr><tr><td>eval_precision</td><td>▁▆██</td></tr><tr><td>eval_recall</td><td>▁▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>0.89325</td></tr><tr><td>eval_f1</td><td>0.71098</td></tr><tr><td>eval_precision</td><td>0.70196</td></tr><tr><td>eval_recall</td><td>0.72024</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"You can sync this run to the cloud by running:<br><code>wandb sync /kaggle/working/wandb/offline-run-20260107_190102-ejs587jx<code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/offline-run-20260107_190102-ejs587jx/logs</code>"},"metadata":{}},{"name":"stdout","text":"\u001b[32m[I 2026-01-07 19:06:57,954]\u001b[0m Trial 32 finished with value: 0.7109805361733381 and parameters: {'learning_rate': 2.7544430790280434e-05, 'batch_size': 32, 'num_train_epochs': 3, 'temperature': 1.4266961307368469, 'alpha': 0.10459523676415396}. Best is trial 32 with value: 0.7109805361733381.\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.22.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/kaggle/working/wandb/offline-run-20260107_190657-h6bbwcx9</code>"},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 16.982, 'grad_norm': 92.34516143798828, 'learning_rate': 2.3735440958676395e-05, 'epoch': 0.4}\n{'loss': 8.0262, 'grad_norm': 115.52241516113281, 'learning_rate': 2.0095035903664678e-05, 'epoch': 0.8}\n{'eval_loss': 4.7022786140441895, 'eval_precision': 0.3902439024390244, 'eval_recall': 0.44047619047619047, 'eval_f1': 0.4138413142257952, 'eval_accuracy': 0.7586551264980027, 'eval_runtime': 16.9858, 'eval_samples_per_second': 58.873, 'eval_steps_per_second': 7.359, 'epoch': 1.0}\n{'loss': 5.3743, 'grad_norm': 84.32349395751953, 'learning_rate': 1.6454630848652964e-05, 'epoch': 1.2}\n{'loss': 4.0982, 'grad_norm': 118.26274871826172, 'learning_rate': 1.2814225793641245e-05, 'epoch': 1.6}\n{'loss': 3.6937, 'grad_norm': 323.6283264160156, 'learning_rate': 9.173820738629528e-06, 'epoch': 2.0}\n{'eval_loss': 2.914287805557251, 'eval_precision': 0.6291486291486291, 'eval_recall': 0.6488095238095238, 'eval_f1': 0.6388278388278389, 'eval_accuracy': 0.8711717709720372, 'eval_runtime': 16.9911, 'eval_samples_per_second': 58.854, 'eval_steps_per_second': 7.357, 'epoch': 2.0}\n{'loss': 2.8258, 'grad_norm': 58.4005012512207, 'learning_rate': 5.53341568361781e-06, 'epoch': 2.4}\n{'loss': 2.6517, 'grad_norm': 65.64393615722656, 'learning_rate': 1.8930106286060929e-06, 'epoch': 2.8}\n{'eval_loss': 2.4394400119781494, 'eval_precision': 0.6836441893830703, 'eval_recall': 0.7090773809523809, 'eval_f1': 0.6961285609934258, 'eval_accuracy': 0.9014647137150466, 'eval_runtime': 16.9764, 'eval_samples_per_second': 58.905, 'eval_steps_per_second': 7.363, 'epoch': 3.0}\n{'train_runtime': 330.5098, 'train_samples_per_second': 36.308, 'train_steps_per_second': 1.135, 'train_loss': 5.998733703613281, 'epoch': 3.0}\n{'eval_loss': 2.4394400119781494, 'eval_precision': 0.6836441893830703, 'eval_recall': 0.7090773809523809, 'eval_f1': 0.6961285609934258, 'eval_accuracy': 0.9014647137150466, 'eval_runtime': 16.9683, 'eval_samples_per_second': 58.933, 'eval_steps_per_second': 7.367, 'epoch': 3.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>▁▇██</td></tr><tr><td>eval_f1</td><td>▁▇██</td></tr><tr><td>eval_precision</td><td>▁▇██</td></tr><tr><td>eval_recall</td><td>▁▆██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>0.90146</td></tr><tr><td>eval_f1</td><td>0.69613</td></tr><tr><td>eval_precision</td><td>0.68364</td></tr><tr><td>eval_recall</td><td>0.70908</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"You can sync this run to the cloud by running:<br><code>wandb sync /kaggle/working/wandb/offline-run-20260107_190657-h6bbwcx9<code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/offline-run-20260107_190657-h6bbwcx9/logs</code>"},"metadata":{}},{"name":"stdout","text":"\u001b[32m[I 2026-01-07 19:12:54,056]\u001b[0m Trial 33 finished with value: 0.6961285609934258 and parameters: {'learning_rate': 2.730303791258788e-05, 'batch_size': 32, 'num_train_epochs': 3, 'temperature': 1.3735746334392411, 'alpha': 0.10527320522644483}. Best is trial 32 with value: 0.7109805361733381.\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.22.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/kaggle/working/wandb/offline-run-20260107_191254-8u17nwh5</code>"},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 17.0061, 'grad_norm': 93.5002212524414, 'learning_rate': 2.4028933599921175e-05, 'epoch': 0.4}\n{'loss': 7.9321, 'grad_norm': 142.9009246826172, 'learning_rate': 2.0343514336129583e-05, 'epoch': 0.8}\n{'eval_loss': 4.714171886444092, 'eval_precision': 0.3743654822335025, 'eval_recall': 0.43898809523809523, 'eval_f1': 0.40410958904109584, 'eval_accuracy': 0.7548823790501553, 'eval_runtime': 16.9919, 'eval_samples_per_second': 58.852, 'eval_steps_per_second': 7.356, 'epoch': 1.0}\n{'loss': 5.581, 'grad_norm': 74.70557403564453, 'learning_rate': 1.6658095072337995e-05, 'epoch': 1.2}\n{'loss': 4.3193, 'grad_norm': 175.72760009765625, 'learning_rate': 1.2972675808546401e-05, 'epoch': 1.6}\n{'loss': 3.9578, 'grad_norm': 158.7412872314453, 'learning_rate': 9.287256544754811e-06, 'epoch': 2.0}\n{'eval_loss': 2.953604221343994, 'eval_precision': 0.5964539007092199, 'eval_recall': 0.6257440476190477, 'eval_f1': 0.6107480029048657, 'eval_accuracy': 0.8405459387483355, 'eval_runtime': 16.9656, 'eval_samples_per_second': 58.943, 'eval_steps_per_second': 7.368, 'epoch': 2.0}\n{'loss': 2.9731, 'grad_norm': 75.56507873535156, 'learning_rate': 5.601837280963219e-06, 'epoch': 2.4}\n{'loss': 2.7905, 'grad_norm': 107.77780151367188, 'learning_rate': 1.9164180171716275e-06, 'epoch': 2.8}\n{'eval_loss': 2.446518659591675, 'eval_precision': 0.669296987087518, 'eval_recall': 0.6941964285714286, 'eval_f1': 0.6815193571950329, 'eval_accuracy': 0.8931424766977364, 'eval_runtime': 16.9475, 'eval_samples_per_second': 59.006, 'eval_steps_per_second': 7.376, 'epoch': 3.0}\n{'train_runtime': 330.5214, 'train_samples_per_second': 36.306, 'train_steps_per_second': 1.135, 'train_loss': 6.135072672526042, 'epoch': 3.0}\n{'eval_loss': 2.446518659591675, 'eval_precision': 0.669296987087518, 'eval_recall': 0.6941964285714286, 'eval_f1': 0.6815193571950329, 'eval_accuracy': 0.8931424766977364, 'eval_runtime': 16.9408, 'eval_samples_per_second': 59.029, 'eval_steps_per_second': 7.379, 'epoch': 3.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>▁▅██</td></tr><tr><td>eval_f1</td><td>▁▆██</td></tr><tr><td>eval_precision</td><td>▁▆██</td></tr><tr><td>eval_recall</td><td>▁▆██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>0.89314</td></tr><tr><td>eval_f1</td><td>0.68152</td></tr><tr><td>eval_precision</td><td>0.6693</td></tr><tr><td>eval_recall</td><td>0.6942</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"You can sync this run to the cloud by running:<br><code>wandb sync /kaggle/working/wandb/offline-run-20260107_191254-8u17nwh5<code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/offline-run-20260107_191254-8u17nwh5/logs</code>"},"metadata":{}},{"name":"stdout","text":"\u001b[32m[I 2026-01-07 19:18:50,082]\u001b[0m Trial 34 finished with value: 0.6815193571950329 and parameters: {'learning_rate': 2.7640644478436936e-05, 'batch_size': 32, 'num_train_epochs': 3, 'temperature': 1.4157773455842484, 'alpha': 0.102343123590297}. Best is trial 32 with value: 0.7109805361733381.\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.22.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/kaggle/working/wandb/offline-run-20260107_191850-vmpbi5fs</code>"},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 27.587, 'grad_norm': 167.64247131347656, 'learning_rate': 1.8970697796412147e-05, 'epoch': 0.4}\n{'loss': 11.4097, 'grad_norm': 181.44503784179688, 'learning_rate': 1.6061081569968568e-05, 'epoch': 0.8}\n{'eval_loss': 6.669497489929199, 'eval_precision': 0.4759679572763685, 'eval_recall': 0.5305059523809523, 'eval_f1': 0.501759324419423, 'eval_accuracy': 0.772525521526853, 'eval_runtime': 16.9586, 'eval_samples_per_second': 58.967, 'eval_steps_per_second': 7.371, 'epoch': 1.0}\n{'loss': 7.5087, 'grad_norm': 129.43927001953125, 'learning_rate': 1.3151465343524986e-05, 'epoch': 1.2}\n{'loss': 6.2013, 'grad_norm': 193.95480346679688, 'learning_rate': 1.0241849117081405e-05, 'epoch': 1.6}\n{'loss': 5.7674, 'grad_norm': 527.5787353515625, 'learning_rate': 7.332232890637825e-06, 'epoch': 2.0}\n{'eval_loss': 4.487786769866943, 'eval_precision': 0.6533333333333333, 'eval_recall': 0.65625, 'eval_f1': 0.6547884187082404, 'eval_accuracy': 0.8311140701287173, 'eval_runtime': 16.9467, 'eval_samples_per_second': 59.008, 'eval_steps_per_second': 7.376, 'epoch': 2.0}\n{'loss': 4.567, 'grad_norm': 69.2181625366211, 'learning_rate': 4.4226166641942426e-06, 'epoch': 2.4}\n{'loss': 4.4252, 'grad_norm': 125.08084869384766, 'learning_rate': 1.513000437750662e-06, 'epoch': 2.8}\n{'eval_loss': 4.031238079071045, 'eval_precision': 0.6844840386043058, 'eval_recall': 0.6860119047619048, 'eval_f1': 0.6852471200297288, 'eval_accuracy': 0.8637372392365734, 'eval_runtime': 16.9537, 'eval_samples_per_second': 58.984, 'eval_steps_per_second': 7.373, 'epoch': 3.0}\n{'train_runtime': 330.3082, 'train_samples_per_second': 36.33, 'train_steps_per_second': 1.135, 'train_loss': 9.286645446777344, 'epoch': 3.0}\n{'eval_loss': 4.031238079071045, 'eval_precision': 0.6844840386043058, 'eval_recall': 0.6860119047619048, 'eval_f1': 0.6852471200297288, 'eval_accuracy': 0.8637372392365734, 'eval_runtime': 16.9495, 'eval_samples_per_second': 58.999, 'eval_steps_per_second': 7.375, 'epoch': 3.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>▁▅██</td></tr><tr><td>eval_f1</td><td>▁▇██</td></tr><tr><td>eval_precision</td><td>▁▇██</td></tr><tr><td>eval_recall</td><td>▁▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>0.86374</td></tr><tr><td>eval_f1</td><td>0.68525</td></tr><tr><td>eval_precision</td><td>0.68448</td></tr><tr><td>eval_recall</td><td>0.68601</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"You can sync this run to the cloud by running:<br><code>wandb sync /kaggle/working/wandb/offline-run-20260107_191850-vmpbi5fs<code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/offline-run-20260107_191850-vmpbi5fs/logs</code>"},"metadata":{}},{"name":"stdout","text":"\u001b[32m[I 2026-01-07 19:24:45,982]\u001b[0m Trial 35 finished with value: 0.6852471200297288 and parameters: {'learning_rate': 2.1822121698326857e-05, 'batch_size': 32, 'num_train_epochs': 3, 'temperature': 1.55594908790464, 'alpha': 0.152019819127799}. Best is trial 32 with value: 0.7109805361733381.\u001b[0m\n","output_type":"stream"}],"execution_count":102},{"cell_type":"code","source":"# Cell 8: Display optimization results\nprint(\"Optimization completed!\")\nprint(f\"Number of finished trials: {len(study.trials)}\")\n\nprint(\"\\nBest trial:\")\ntrial = study.best_trial\n\nprint(f\"  Value (F1 Score): {trial.value:.4f}\")\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T19:26:19.484131Z","iopub.execute_input":"2026-01-07T19:26:19.484704Z","iopub.status.idle":"2026-01-07T19:26:19.504901Z","shell.execute_reply.started":"2026-01-07T19:26:19.484678Z","shell.execute_reply":"2026-01-07T19:26:19.504374Z"}},"outputs":[{"name":"stdout","text":"Optimization completed!\nNumber of finished trials: 36\n\nBest trial:\n  Value (F1 Score): 0.7110\n  Params: \n    alpha: 0.10459523676415396\n    batch_size: 32\n    learning_rate: 2.7544430790280434e-05\n    num_train_epochs: 3\n    temperature: 1.4266961307368469\n","output_type":"stream"}],"execution_count":103},{"cell_type":"code","source":"# Cell 9: Save best parameters\nbest_params = {\n    \"best_value\": trial.value,\n    \"params\": trial.params,\n    \"datetime\": str(np.datetime64('now'))\n}\n\nos.makedirs(config.OUTPUT_DIR, exist_ok=True)\nwith open(os.path.join(config.OUTPUT_DIR, \"best_params.json\"), \"w\") as f:\n    json.dump(best_params, f, indent=2)\n\nprint(f\"\\nBest parameters saved to {config.OUTPUT_DIR}/best_params.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T19:26:29.768434Z","iopub.execute_input":"2026-01-07T19:26:29.769172Z","iopub.status.idle":"2026-01-07T19:26:29.774871Z","shell.execute_reply.started":"2026-01-07T19:26:29.769146Z","shell.execute_reply":"2026-01-07T19:26:29.774231Z"}},"outputs":[{"name":"stdout","text":"\nBest parameters saved to ./models/optuna_tuned/best_params.json\n","output_type":"stream"}],"execution_count":104},{"cell_type":"code","source":"!pip install -U kaleido","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T19:26:56.053890Z","iopub.execute_input":"2026-01-07T19:26:56.054284Z","iopub.status.idle":"2026-01-07T19:26:59.623914Z","shell.execute_reply.started":"2026-01-07T19:26:56.054258Z","shell.execute_reply":"2026-01-07T19:26:59.623224Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: kaleido in /usr/local/lib/python3.12/dist-packages (1.2.0)\nRequirement already satisfied: choreographer>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from kaleido) (1.2.1)\nRequirement already satisfied: logistro>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from kaleido) (2.0.1)\nRequirement already satisfied: orjson>=3.10.15 in /usr/local/lib/python3.12/dist-packages (from kaleido) (3.11.3)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kaleido) (25.0)\nRequirement already satisfied: pytest-timeout>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from kaleido) (2.4.0)\nRequirement already satisfied: simplejson>=3.19.3 in /usr/local/lib/python3.12/dist-packages (from choreographer>=1.1.1->kaleido) (3.20.2)\nRequirement already satisfied: pytest>=7.0.0 in /usr/local/lib/python3.12/dist-packages (from pytest-timeout>=2.4.0->kaleido) (8.4.2)\nRequirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (2.3.0)\nRequirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (1.6.0)\nRequirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (2.19.2)\n","output_type":"stream"}],"execution_count":106},{"cell_type":"code","source":"# Cell 10: Visualization 1 - Optimization History\nimport plotly.io as pio\n\ndef plot_optimization_history(study):\n    # Set the renderer to work specifically in Kaggle/Colab environments\n    pio.renderers.default = \"notebook_connected\" \n    \n    fig = optuna.visualization.plot_optimization_history(study)\n    fig.update_layout(\n        title=\"Optimization History\",\n        xaxis_title=\"Trial\",\n        yaxis_title=\"F1 Score\",\n        template=\"plotly_white\", # Force a clean theme\n        width=900,\n        height=500\n    )\n    \n    # Use 'iframe' if 'notebook_connected' still shows white\n    fig.show(renderer=\"iframe\") \n\nplot_optimization_history(study)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T19:29:59.815575Z","iopub.execute_input":"2026-01-07T19:29:59.815883Z","iopub.status.idle":"2026-01-07T19:30:00.850407Z","shell.execute_reply.started":"2026-01-07T19:29:59.815858Z","shell.execute_reply":"2026-01-07T19:30:00.849742Z"}},"outputs":[{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"920px\"\n    height=\"520\"\n    src=\"iframe_figures/figure_110.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}}],"execution_count":110},{"cell_type":"code","source":"# Cell 11: Visualization 2 - Parameter Importances\nimport plotly.io as pio\nimport optuna\n\ndef plot_param_importances(study):\n    # 1. Set renderer to fix the \"White Screen\" issue in Kaggle\n    pio.renderers.default = \"notebook_connected\"\n    \n    try:\n        # 2. Use a different evaluator to avoid the \"array element with a sequence\" error\n        # MeanDecreaseImpurity is more stable than the default fANOVA\n        evaluator = optuna.importance.MeanDecreaseImpurityImportanceEvaluator()\n        \n        fig = optuna.visualization.plot_param_importances(study, evaluator=evaluator)\n        \n        fig.update_layout(\n            title=\"Parameter Importances (Mean Decrease Impurity)\",\n            template=\"plotly_white\",\n            width=800,\n            height=500\n        )\n        \n        # 3. Use iframe renderer for reliable display in Kaggle\n        fig.show(renderer=\"iframe\")\n        \n    except Exception as e:\n        print(f\"Could not plot importance: {e}\")\n        print(\"This often happens if there are too few successful trials (score > 0).\")\n        print(\"Current successful trials:\", len([t for t in study.trials if t.state.name == 'COMPLETE' and t.value > 0]))\n\nplot_param_importances(study)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T19:31:17.546497Z","iopub.execute_input":"2026-01-07T19:31:17.546797Z","iopub.status.idle":"2026-01-07T19:31:17.732427Z","shell.execute_reply.started":"2026-01-07T19:31:17.546774Z","shell.execute_reply":"2026-01-07T19:31:17.731666Z"}},"outputs":[{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"820px\"\n    height=\"520\"\n    src=\"iframe_figures/figure_111.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}}],"execution_count":111},{"cell_type":"code","source":"# Cell 12: Visualization 3 - Parallel Coordinate Plot\nimport plotly.io as pio\n\ndef plot_parallel_coordinate(study):\n    # Set the renderer for Kaggle compatibility\n    pio.renderers.default = \"notebook_connected\"\n    \n    # Check which parameters actually exist in the study to avoid ValueErrors\n    # This automatically finds whatever you suggested (alpha, temperature, etc.)\n    existing_params = list(study.best_params.keys())\n    \n    fig = optuna.visualization.plot_parallel_coordinate(\n        study,\n        params=existing_params\n    )\n    \n    fig.update_layout(\n        title=\"Hyperparameter Interactions (Parallel Coordinate)\",\n        template=\"plotly_white\",\n        width=1100,\n        height=600\n    )\n    \n    # Save image commented out unless Kaleido is fixed\n    # fig.write_image(os.path.join(config.OUTPUT_DIR, \"parallel_coordinate.png\"))\n    \n    # Show using iframe to prevent the \"White Screen\" issue\n    fig.show(renderer=\"iframe\")\n\nplot_parallel_coordinate(study)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T19:32:16.040184Z","iopub.execute_input":"2026-01-07T19:32:16.040930Z","iopub.status.idle":"2026-01-07T19:32:16.120871Z","shell.execute_reply.started":"2026-01-07T19:32:16.040892Z","shell.execute_reply":"2026-01-07T19:32:16.120113Z"}},"outputs":[{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"1120px\"\n    height=\"620\"\n    src=\"iframe_figures/figure_113.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}}],"execution_count":113},{"cell_type":"code","source":"# Cell 13: Visualization 4 - Slice Plot\nimport plotly.io as pio\nimport os\n\ndef plot_slice_plot(study):\n    # Set the renderer for Kaggle compatibility\n    pio.renderers.default = \"notebook_connected\"\n    \n    # Automatically get parameters that actually exist in the study\n    # This prevents \"Parameter X does not exist\" errors\n    existing_params = list(study.best_params.keys())\n    \n    fig = optuna.visualization.plot_slice(\n        study,\n        params=existing_params\n    )\n    \n    fig.update_layout(\n        title=\"Slice Plot (Parameter Relationships with F1)\",\n        template=\"plotly_white\",\n        width=1200, # Increased width to give each slice more room\n        height=600\n    )\n    \n    # Commented out to prevent the Kaleido/Image export error\n    # if not os.path.exists(config.OUTPUT_DIR):\n    #     os.makedirs(config.OUTPUT_DIR)\n    # fig.write_image(os.path.join(config.OUTPUT_DIR, \"slice_plot.png\"))\n    \n    # Show using iframe renderer to ensure it displays in Kaggle\n    fig.show(renderer=\"iframe\")\n\nplot_slice_plot(study)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T19:33:18.152309Z","iopub.execute_input":"2026-01-07T19:33:18.152960Z","iopub.status.idle":"2026-01-07T19:33:18.239177Z","shell.execute_reply.started":"2026-01-07T19:33:18.152934Z","shell.execute_reply":"2026-01-07T19:33:18.238570Z"}},"outputs":[{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"1220px\"\n    height=\"620\"\n    src=\"iframe_figures/figure_116.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}}],"execution_count":116},{"cell_type":"code","source":"# Cell 14: Create interactive dashboard\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nfrom optuna.trial import TrialState\nimport os\n\ndef create_interactive_dashboard(study):\n    # Set the renderer for Kaggle compatibility\n    pio.renderers.default = \"notebook_connected\"\n    \n    # 1. Get trial data\n    trials = study.trials\n    \n    # 2. Identify parameters that were actually used in the study\n    # This prevents the script from crashing if 'weight_decay' or others are missing\n    all_params = set()\n    for t in trials:\n        if t.state == TrialState.COMPLETE:\n            all_params.update(t.params.keys())\n    param_names = list(all_params)\n    \n    # 3. Create DataFrame for plotting\n    data = []\n    for trial in trials:\n        if trial.state == TrialState.COMPLETE:\n            # We want to handle cases where f1_score might be None\n            score = trial.value if trial.value is not None else 0.0\n            row = {\"trial\": trial.number, \"f1_score\": score}\n            for param in param_names:\n                row[param] = trial.params.get(param, None)\n            data.append(row)\n    \n    if not data:\n        print(\"No completed trials found to plot.\")\n        return pd.DataFrame()\n\n    df = pd.DataFrame(data)\n    \n    # 4. Create interactive scatter matrix\n    # Using Plotly Express (px)\n    fig = px.scatter_matrix(\n        df,\n        dimensions=param_names,\n        color=\"f1_score\",\n        symbol=\"f1_score\", # Adds variation to points\n        title=\"Hyperparameter Relationships (Scatter Matrix)\",\n        labels={col: col.replace('_', ' ') for col in df.columns}, # Clean up axis labels\n        width=1200,\n        height=800,\n        color_continuous_scale=px.colors.sequential.Viridis\n    )\n    \n    # Improve readability by reducing marker size and hiding diagonal labels\n    fig.update_traces(diagonal_visible=False, marker=dict(size=5))\n    \n    # Commented out to prevent the Kaleido error\n    # fig.write_image(os.path.join(config.OUTPUT_DIR, \"scatter_matrix.png\"))\n    \n    # Use iframe renderer for reliable display\n    fig.show(renderer=\"iframe\")\n    \n    return df\n\ndf_trials = create_interactive_dashboard(study)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T19:33:57.453357Z","iopub.execute_input":"2026-01-07T19:33:57.454118Z","iopub.status.idle":"2026-01-07T19:33:57.680039Z","shell.execute_reply.started":"2026-01-07T19:33:57.454076Z","shell.execute_reply":"2026-01-07T19:33:57.679430Z"}},"outputs":[{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"1220px\"\n    height=\"820\"\n    src=\"iframe_figures/figure_117.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}}],"execution_count":117},{"cell_type":"code","source":"# Cell 15: Train final model with best parameters\nimport json\nimport os\nimport torch\nimport numpy as np\nfrom datasets import Dataset, DatasetDict, load_dataset\nfrom transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n\ndef train_final_model(best_trial, full_dataset=True):\n    print(\"\\nTraining final model with best parameters...\")\n    \n    # Extract params from the trial object\n    params = best_trial.params\n    \n    # Load/Combine dataset logic\n    if full_dataset:\n        print(\"Loading full multilingual dataset...\")\n        combined_data = {'train': [], 'validation': []}\n        for lang in [\"en\", \"de\", \"fr\", \"es\"]:\n            try:\n                ds = load_dataset(config.DATASET_NAME, lang)\n                combined_data['train'].extend(ds['train'])\n                combined_data['validation'].extend(ds['validation'])\n            except Exception as e:\n                print(f\"Could not load {lang}: {e}\")\n                continue\n        \n        # Convert lists back to Hugging Face Datasets\n        final_dataset = DatasetDict({\n            'train': Dataset.from_list(combined_data['train']),\n            'validation': Dataset.from_list(combined_data['validation'])\n        })\n    else:\n        # Use the tokenized_dataset already in memory\n        final_dataset = dataset \n\n    # Tokenize (using the same function from Cell 2)\n    tokenized_final = final_dataset.map(\n        tokenize_and_align_labels,\n        batched=True,\n        remove_columns=final_dataset[\"train\"].column_names\n    )\n    \n    # Initialize model with correct label mappings\n    model = AutoModelForTokenClassification.from_pretrained(\n        config.STUDENT_MODEL,\n        num_labels=config.NUM_LABELS,\n        id2label={i: label for i, label in enumerate(config.LABEL_NAMES)},\n        label2id={label: i for i, label in enumerate(config.LABEL_NAMES)}\n    )\n    \n    # Training arguments with best params + safety defaults\n    training_args = TrainingArguments(\n        output_dir=os.path.join(config.OUTPUT_DIR, \"final_model\"),\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        learning_rate=params[\"learning_rate\"],\n        per_device_train_batch_size=params[\"batch_size\"],\n        per_device_eval_batch_size=params[\"batch_size\"] * 2,\n        num_train_epochs=params[\"num_train_epochs\"],\n        weight_decay=params.get(\"weight_decay\", 0.01), # Default if not tuned\n        warmup_ratio=params.get(\"warmup_ratio\", 0.1),  # Default if not tuned\n        logging_dir=os.path.join(config.OUTPUT_DIR, \"logs_final\"),\n        logging_steps=50,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_f1\",\n        greater_is_better=True,\n        report_to=\"none\",\n        save_total_limit=1,\n        fp16=torch.cuda.is_available(),\n    )\n    \n    # Metrics\n    from evaluate import load\n    seqeval = load(\"seqeval\")\n    \n    def compute_metrics(p):\n        predictions, labels = p\n        predictions = np.argmax(predictions, axis=2)\n        true_predictions = [\n            [config.LABEL_NAMES[p] for (p, l) in zip(prediction, label) if l != -100]\n            for prediction, label in zip(predictions, labels)\n        ]\n        true_labels = [\n            [config.LABEL_NAMES[l] for (p, l) in zip(prediction, label) if l != -100]\n            for prediction, label in zip(predictions, labels)\n        ]\n        results = seqeval.compute(predictions=true_predictions, references=true_labels)\n        return {\n            \"f1\": results[\"overall_f1\"],\n            \"precision\": results[\"overall_precision\"],\n            \"recall\": results[\"overall_recall\"],\n            \"accuracy\": results[\"overall_accuracy\"]\n        }\n    \n    # IMPORTANT: Use DistillationTrainer for the final run to keep the benefits!\n    trainer = DistillationTrainer(\n        model=model,\n        teacher_model=teacher_model,\n        temperature=params[\"temperature\"],\n        alpha=params[\"alpha\"],\n        args=training_args,\n        train_dataset=tokenized_final[\"train\"],\n        eval_dataset=tokenized_final[\"validation\"],\n        tokenizer=tokenizer,\n        data_collator=DataCollatorForTokenClassification(tokenizer),\n        compute_metrics=compute_metrics,\n    )\n    \n    # Train and Evaluate\n    trainer.train()\n    final_eval = trainer.evaluate()\n    \n    # Save everything\n    os.makedirs(os.path.join(config.OUTPUT_DIR, \"final_model\"), exist_ok=True)\n    trainer.save_model()\n    tokenizer.save_pretrained(os.path.join(config.OUTPUT_DIR, \"final_model\"))\n    \n    # Save metrics JSON\n    with open(os.path.join(config.OUTPUT_DIR, \"final_model\", \"training_metrics.json\"), \"w\") as f:\n        json.dump(final_eval, f, indent=2)\n    \n    print(f\"✅ Final model saved with F1: {final_eval['eval_f1']:.4f}\")\n    return model, trainer, final_eval\n\n# --- EXECUTION ---\n# Pass the best trial from your study\nfinal_model, final_trainer, final_metrics = train_final_model(study.best_trial, full_dataset=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T19:35:04.388287Z","iopub.execute_input":"2026-01-07T19:35:04.388604Z","iopub.status.idle":"2026-01-07T19:41:10.895867Z","shell.execute_reply.started":"2026-01-07T19:35:04.388578Z","shell.execute_reply":"2026-01-07T19:41:10.894677Z"}},"outputs":[{"name":"stdout","text":"\nTraining final model with best parameters...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b8da9ee09054d99bee3fe3e4b333d22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b49d63b8158949548b2e02b67913d92f"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [375/375 05:44, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>8.227400</td>\n      <td>4.788662</td>\n      <td>0.429593</td>\n      <td>0.413343</td>\n      <td>0.447173</td>\n      <td>0.765202</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.598700</td>\n      <td>2.836839</td>\n      <td>0.658617</td>\n      <td>0.647948</td>\n      <td>0.669643</td>\n      <td>0.884376</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.520300</td>\n      <td>2.318077</td>\n      <td>0.717724</td>\n      <td>0.703863</td>\n      <td>0.732143</td>\n      <td>0.910786</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [16/16 00:14]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"✅ Final model saved with F1: 0.7177\n","output_type":"stream"}],"execution_count":118},{"cell_type":"code","source":"# Cell 16: Analyze hyperparameter relationships\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.io as pio\nimport os\n\ndef analyze_hyperparameter_relationships(df_trials):\n    # Set the renderer for Kaggle compatibility\n    pio.renderers.default = \"notebook_connected\"\n    \n    # 1. Prepare data - Exclude 'trial' number from correlation as it's just an index\n    # We only want to see how hyperparameters correlate with f1_score\n    numeric_cols = df_trials.select_dtypes(include=[np.number]).columns\n    cols_to_corr = [col for col in numeric_cols if col != 'trial']\n    corr_matrix = df_trials[cols_to_corr].corr()\n    \n    # 2. Create correlation heatmap\n    fig = go.Figure(data=go.Heatmap(\n        z=corr_matrix.values,\n        x=corr_matrix.columns,\n        y=corr_matrix.columns,\n        colorscale='RdBu',\n        zmin=-1, zmax=1, # Correlations are always between -1 and 1\n        zmid=0,\n        text=corr_matrix.round(2).values,\n        texttemplate='%{text}',\n        textfont={\"size\": 12}\n    ))\n    \n    fig.update_layout(\n        title=\"Hyperparameter & F1 Score Correlation Matrix\",\n        width=700,\n        height=600,\n        template=\"plotly_white\"\n    )\n    \n    # Commented out to prevent Kaleido error\n    # fig.write_image(os.path.join(config.OUTPUT_DIR, \"correlation_matrix.png\"))\n    \n    # Show using iframe for Kaggle\n    fig.show(renderer=\"iframe\")\n    \n    # 3. Analyze best performing regions\n    print(\"\\n\" + \"=\"*30)\n    print(\"TOP 5 PERFORMING TRIALS\")\n    print(\"=\"*30)\n    \n    # Dynamically find parameter columns (everything except trial and f1_score)\n    actual_params = [col for col in df_trials.columns if col not in ['trial', 'f1_score']]\n    \n    top_trials = df_trials.nlargest(5, 'f1_score')\n    # Use format strings for cleaner output\n    print(top_trials[['trial', 'f1_score'] + actual_params].to_string(index=False))\n    \n    return corr_matrix\n\n# Execute the analysis\ncorr_matrix = analyze_hyperparameter_relationships(df_trials)\ncorr_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T19:41:10.897483Z","iopub.execute_input":"2026-01-07T19:41:10.898072Z","iopub.status.idle":"2026-01-07T19:41:10.977610Z","shell.execute_reply.started":"2026-01-07T19:41:10.898030Z","shell.execute_reply":"2026-01-07T19:41:10.977015Z"}},"outputs":[{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"720px\"\n    height=\"620\"\n    src=\"iframe_figures/figure_119.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}},{"name":"stdout","text":"\n==============================\nTOP 5 PERFORMING TRIALS\n==============================\n trial  f1_score  num_train_epochs  learning_rate    alpha  temperature  batch_size\n    32  0.710981                 3       0.000028 0.104595     1.426696          32\n    33  0.696129                 3       0.000027 0.105273     1.373575          32\n    30  0.695716                 3       0.000024 0.154465     1.516702          32\n    29  0.690691                 3       0.000024 0.176908     1.489960          32\n    27  0.687547                 3       0.000023 0.195373     1.130070          32\n","output_type":"stream"},{"execution_count":119,"output_type":"execute_result","data":{"text/plain":"                  f1_score  num_train_epochs  learning_rate     alpha  \\\nf1_score          1.000000         -0.029616       0.007540 -0.606878   \nnum_train_epochs -0.029616          1.000000      -0.171527  0.115405   \nlearning_rate     0.007540         -0.171527       1.000000 -0.066335   \nalpha            -0.606878          0.115405      -0.066335  1.000000   \ntemperature      -0.604042          0.269889       0.004513  0.425231   \nbatch_size        0.454241          0.313696       0.066193 -0.523405   \n\n                  temperature  batch_size  \nf1_score            -0.604042    0.454241  \nnum_train_epochs     0.269889    0.313696  \nlearning_rate        0.004513    0.066193  \nalpha                0.425231   -0.523405  \ntemperature          1.000000   -0.114710  \nbatch_size          -0.114710    1.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f1_score</th>\n      <th>num_train_epochs</th>\n      <th>learning_rate</th>\n      <th>alpha</th>\n      <th>temperature</th>\n      <th>batch_size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>f1_score</th>\n      <td>1.000000</td>\n      <td>-0.029616</td>\n      <td>0.007540</td>\n      <td>-0.606878</td>\n      <td>-0.604042</td>\n      <td>0.454241</td>\n    </tr>\n    <tr>\n      <th>num_train_epochs</th>\n      <td>-0.029616</td>\n      <td>1.000000</td>\n      <td>-0.171527</td>\n      <td>0.115405</td>\n      <td>0.269889</td>\n      <td>0.313696</td>\n    </tr>\n    <tr>\n      <th>learning_rate</th>\n      <td>0.007540</td>\n      <td>-0.171527</td>\n      <td>1.000000</td>\n      <td>-0.066335</td>\n      <td>0.004513</td>\n      <td>0.066193</td>\n    </tr>\n    <tr>\n      <th>alpha</th>\n      <td>-0.606878</td>\n      <td>0.115405</td>\n      <td>-0.066335</td>\n      <td>1.000000</td>\n      <td>0.425231</td>\n      <td>-0.523405</td>\n    </tr>\n    <tr>\n      <th>temperature</th>\n      <td>-0.604042</td>\n      <td>0.269889</td>\n      <td>0.004513</td>\n      <td>0.425231</td>\n      <td>1.000000</td>\n      <td>-0.114710</td>\n    </tr>\n    <tr>\n      <th>batch_size</th>\n      <td>0.454241</td>\n      <td>0.313696</td>\n      <td>0.066193</td>\n      <td>-0.523405</td>\n      <td>-0.114710</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":119},{"cell_type":"code","source":"# Cell 17: Export optimization report\nimport json\nimport os\nimport numpy as np\n\ndef export_optimization_report(study, df_trials, config):\n    # Safety Check: If no trials completed successfully, df_trials might be empty\n    if df_trials.empty:\n        print(\"❌ Error: No completed trials found. Cannot export report.\")\n        return None\n\n    # Handle datetime conversion safely\n    from datetime import datetime\n    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    report = {\n        \"study_info\": {\n            \"study_name\": getattr(study, \"study_name\", \"optuna_study\"),\n            \"n_trials\": len(study.trials),\n            \"best_value\": study.best_value if study.best_trials else None,\n            \"best_params\": study.best_params,\n            \"datetime\": current_time\n        },\n        \"config\": {\n            \"student_model\": config.STUDENT_MODEL,\n            \"languages\": config.LANGUAGES,\n            \"n_trials\": config.N_TRIALS\n        },\n        \"trials_summary\": {\n            \"mean_f1\": float(df_trials['f1_score'].mean()),\n            \"std_f1\": float(df_trials['f1_score'].std()) if len(df_trials) > 1 else 0.0,\n            \"min_f1\": float(df_trials['f1_score'].min()),\n            \"max_f1\": float(df_trials['f1_score'].max())\n        }\n    }\n    \n    # Ensure Output Directory exists\n    os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n    \n    report_path = os.path.join(config.OUTPUT_DIR, \"optimization_report.json\")\n    with open(report_path, \"w\") as f:\n        json.dump(report, f, indent=2)\n    \n    print(f\"\\n✅ Optimization report saved to {report_path}\")\n    \n    # Print summary to console\n    print(\"\\n\" + \"=\"*30)\n    print(\"      OPTIMIZATION SUMMARY      \")\n    print(\"=\"*30)\n    print(f\"Best F1 Score:    {study.best_value:.4f}\")\n    print(f\"Average F1 Score: {df_trials['f1_score'].mean():.4f} ± {df_trials['f1_score'].std():.4f}\")\n    print(f\"Number of trials: {len(study.trials)}\")\n    \n    print(f\"\\nBest parameters found:\")\n    for param, value in study.best_params.items():\n        # Format floats for readability\n        if isinstance(value, float):\n            print(f\"  {param:15}: {value:.6f}\")\n        else:\n            print(f\"  {param:15}: {value}\")\n    print(\"=\"*30)\n    \n    return report\n\n# Execute export\nreport = export_optimization_report(study, df_trials, config)\nreport","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T19:43:30.188581Z","iopub.execute_input":"2026-01-07T19:43:30.189183Z","iopub.status.idle":"2026-01-07T19:43:30.252968Z","shell.execute_reply.started":"2026-01-07T19:43:30.189155Z","shell.execute_reply":"2026-01-07T19:43:30.252300Z"}},"outputs":[{"name":"stdout","text":"\n✅ Optimization report saved to ./models/optuna_tuned/optimization_report.json\n\n==============================\n      OPTIMIZATION SUMMARY      \n==============================\nBest F1 Score:    0.7110\nAverage F1 Score: 0.2048 ± 0.3159\nNumber of trials: 36\n\nBest parameters found:\n  alpha          : 0.104595\n  batch_size     : 32\n  learning_rate  : 0.000028\n  num_train_epochs: 3\n  temperature    : 1.426696\n==============================\n","output_type":"stream"},{"execution_count":120,"output_type":"execute_result","data":{"text/plain":"{'study_info': {'study_name': 'multilingual_ner_distillation',\n  'n_trials': 36,\n  'best_value': 0.7109805361733381,\n  'best_params': {'alpha': 0.10459523676415396,\n   'batch_size': 32,\n   'learning_rate': 2.7544430790280434e-05,\n   'num_train_epochs': 3,\n   'temperature': 1.4266961307368469},\n  'datetime': '2026-01-07 19:43:30'},\n 'config': {'student_model': 'xlm-roberta-base',\n  'languages': ['en', 'de'],\n  'n_trials': 10},\n 'trials_summary': {'mean_f1': 0.20478943392912943,\n  'std_f1': 0.31594063640332054,\n  'min_f1': 0.0,\n  'max_f1': 0.7109805361733381}}"},"metadata":{}}],"execution_count":120},{"cell_type":"code","source":"# Cell 18: Create parameter importance table\nimport optuna\n\ndef create_parameter_importance_table(study):\n    # Use the MeanDecreaseImpurity evaluator to avoid the 'sequence' ValueError\n    # This evaluator uses a Random Forest to determine which params moved the F1 score most\n    evaluator = optuna.importance.MeanDecreaseImpurityImportanceEvaluator()\n    \n    try:\n        importances = optuna.importance.get_param_importances(study, evaluator=evaluator)\n        \n        print(\"\\n\" + \"=\"*40)\n        print(f\"{'PARAMETER':<20} | {'IMPORTANCE SCORE'}\")\n        print(\"=\"*40)\n        \n        # Sort by importance (highest first)\n        sorted_importances = sorted(importances.items(), key=lambda x: x[1], reverse=True)\n        \n        for param, importance in sorted_importances:\n            # Create a small text-based bar chart for quick visual reference\n            bar = \"█\" * int(importance * 20)\n            print(f\"{param:<20} | {importance:.4f}  {bar}\")\n            \n        print(\"=\"*40)\n        print(\"Interpretation: Higher scores mean the parameter had a\")\n        print(\"larger impact on the final F1 score.\")\n        \n        return importances\n\n    except Exception as e:\n        print(f\"❌ Could not calculate importance: {e}\")\n        return {}\n\n# Execute\nimportances = create_parameter_importance_table(study)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T19:45:17.395478Z","iopub.execute_input":"2026-01-07T19:45:17.395774Z","iopub.status.idle":"2026-01-07T19:45:17.491771Z","shell.execute_reply.started":"2026-01-07T19:45:17.395750Z","shell.execute_reply":"2026-01-07T19:45:17.491166Z"}},"outputs":[{"name":"stdout","text":"\n========================================\nPARAMETER            | IMPORTANCE SCORE\n========================================\nalpha                | 0.7244  ██████████████\ntemperature          | 0.1583  ███\nbatch_size           | 0.0809  █\nnum_train_epochs     | 0.0294  \nlearning_rate        | 0.0070  \n========================================\nInterpretation: Higher scores mean the parameter had a\nlarger impact on the final F1 score.\n","output_type":"stream"}],"execution_count":122},{"cell_type":"code","source":"# Cell 19: Save visualizations as HTML for interactive viewing\nimport os\nimport plotly.express as px\nimport optuna\n\ndef save_interactive_visualizations(study, df_trials):\n    # Ensure the output directory exists\n    os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n    \n    print(f\"Generating interactive HTML reports in {config.OUTPUT_DIR}...\")\n\n    # 1. Optimization history\n    try:\n        fig1 = optuna.visualization.plot_optimization_history(study)\n        fig1.write_html(os.path.join(config.OUTPUT_DIR, \"optimization_history.html\"))\n    except Exception as e: print(f\"Skipping history plot: {e}\")\n    \n    # 2. Parallel coordinate\n    try:\n        fig2 = optuna.visualization.plot_parallel_coordinate(study)\n        fig2.write_html(os.path.join(config.OUTPUT_DIR, \"parallel_coordinate.html\"))\n    except Exception as e: print(f\"Skipping parallel coordinate: {e}\")\n    \n    # 3. Slice plot\n    try:\n        fig3 = optuna.visualization.plot_slice(study)\n        fig3.write_html(os.path.join(config.OUTPUT_DIR, \"slice_plot.html\"))\n    except Exception as e: print(f\"Skipping slice plot: {e}\")\n    \n    # 4. Create interactive 3D scatter plot\n    try:\n        # Dynamic check: Only hover over columns that actually exist in the dataframe\n        possible_hover = ['num_train_epochs', 'batch_size', 'weight_decay', 'trial']\n        actual_hover = [col for col in possible_hover if col in df_trials.columns]\n        \n        fig4 = px.scatter_3d(\n            df_trials,\n            x='learning_rate',\n            y='temperature',\n            z='alpha',\n            color='f1_score',\n            size='f1_score', # Use F1 score for size to make good trials stand out\n            hover_data=actual_hover,\n            title='3D Hyperparameter Space Exploration',\n            color_continuous_scale=px.colors.sequential.Viridis\n        )\n        fig4.write_html(os.path.join(config.OUTPUT_DIR, \"3d_scatter.html\"))\n        # Show the 3D plot in the notebook as well\n        fig4.show(renderer=\"iframe\")\n        \n    except Exception as e:\n        print(f\"Skipping 3D scatter plot: {e}\")\n    \n    print(f\"\\n✅ All interactive visualizations saved to {config.OUTPUT_DIR}\")\n\n# Execute the saving process\nsave_interactive_visualizations(study, df_trials)\n\nprint(\"\\n🚀 Hyperparameter tuning completed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T19:45:59.543506Z","iopub.execute_input":"2026-01-07T19:45:59.543800Z","iopub.status.idle":"2026-01-07T19:45:59.790683Z","shell.execute_reply.started":"2026-01-07T19:45:59.543777Z","shell.execute_reply":"2026-01-07T19:45:59.789975Z"}},"outputs":[{"name":"stdout","text":"Generating interactive HTML reports in ./models/optuna_tuned...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"100%\"\n    height=\"545px\"\n    src=\"iframe_figures/figure_124.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}},{"name":"stdout","text":"\n✅ All interactive visualizations saved to ./models/optuna_tuned\n\n🚀 Hyperparameter tuning completed successfully!\n","output_type":"stream"}],"execution_count":124},{"cell_type":"markdown","source":"# notebooks/05_onnx_quantization.ipynb","metadata":{}},{"cell_type":"code","source":"# Cell 1: Setup and imports\n!pip install onnx onnxruntime onnxruntime-tools\n!pip install transformers torch\n!pip install psutil  # For memory monitoring\n\nimport torch\nimport numpy as np\nimport onnx\nimport onnxruntime as ort\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nimport os\nimport json\nimport time\nimport psutil\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set seeds\ndef set_seed(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nset_seed(42)\n\n# Cell 2: Configuration\nclass OptimizationConfig:\n    MODEL_PATH = \"./models/student_distilled\"  # Path to distilled model\n    OUTPUT_DIR = \"./models/optimized\"\n    MAX_LENGTH = 128\n    LABEL_NAMES = [\n        \"O\",\n        \"B-PER\", \"I-PER\",\n        \"B-ORG\", \"I-ORG\",\n        \"B-LOC\", \"I-LOC\"\n    ]\n    NUM_LABELS = 7\n    BATCH_SIZE = 1  # For benchmarking\n    N_ITERATIONS = 100  # For benchmarking\n\nconfig = OptimizationConfig()\n\n# Create output directories\nos.makedirs(config.OUTPUT_DIR, exist_ok=True)\nos.makedirs(os.path.join(config.OUTPUT_DIR, \"onnx\"), exist_ok=True)\nos.makedirs(os.path.join(config.OUTPUT_DIR, \"quantized\"), exist_ok=True)\n\n# Cell 3: Load model and tokenizer\nprint(\"Loading model and tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(config.MODEL_PATH)\nmodel = AutoModelForTokenClassification.from_pretrained(config.MODEL_PATH)\n\n# Set model to eval mode\nmodel.eval()\n\n# Cell 4: Prepare sample input for export\ndef prepare_sample_input():\n    sample_text = \"Apple Inc. is headquartered in Cupertino, California.\"\n    tokens = sample_text.split()\n    \n    inputs = tokenizer(\n        tokens,\n        is_split_into_words=True,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=config.MAX_LENGTH,\n        padding=\"max_length\"\n    )\n    \n    return inputs\n\nsample_inputs = prepare_sample_input()\nprint(f\"Sample input shape: {sample_inputs['input_ids'].shape}\")\n\n# Cell 5: Export to ONNX\ndef export_to_onnx(model, tokenizer, output_path):\n    print(f\"Exporting model to ONNX format...\")\n    \n    # Prepare inputs for tracing\n    dummy_input = (\n        sample_inputs[\"input_ids\"],\n        sample_inputs[\"attention_mask\"]\n    )\n    \n    # Define input names\n    input_names = [\"input_ids\", \"attention_mask\"]\n    output_names = [\"logits\"]\n    \n    # Dynamic axes for variable sequence length\n    dynamic_axes = {\n        'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n        'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n        'logits': {0: 'batch_size', 1: 'sequence_length'}\n    }\n    \n    # Export\n    torch.onnx.export(\n        model,\n        dummy_input,\n        output_path,\n        export_params=True,\n        opset_version=13,\n        do_constant_folding=True,\n        input_names=input_names,\n        output_names=output_names,\n        dynamic_axes=dynamic_axes,\n        verbose=False\n    )\n    \n    print(f\"Model exported to {output_path}\")\n    \n    # Verify ONNX model\n    onnx_model = onnx.load(output_path)\n    onnx.checker.check_model(onnx_model)\n    print(\"ONNX model check passed!\")\n    \n    return onnx_model\n\nonnx_path = os.path.join(config.OUTPUT_DIR, \"onnx\", \"model.onnx\")\nonnx_model = export_to_onnx(model, tokenizer, onnx_path)\n\n# Cell 6: Create ONNX Runtime session\ndef create_onnx_session(onnx_path, providers=None):\n    if providers is None:\n        providers = ['CPUExecutionProvider']\n    \n    session_options = ort.SessionOptions()\n    session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n    session_options.optimized_model_filepath = onnx_path.replace(\".onnx\", \"_optimized.onnx\")\n    \n    session = ort.InferenceSession(\n        onnx_path,\n        sess_options=session_options,\n        providers=providers\n    )\n    \n    print(f\"ONNX Runtime session created with providers: {session.get_providers()}\")\n    return session\n\nonnx_session = create_onnx_session(onnx_path)\n\n# Cell 7: Benchmarking function\nclass ModelBenchmark:\n    def __init__(self, model_name):\n        self.model_name = model_name\n        self.results = {}\n        \n    def measure_memory(self):\n        \"\"\"Measure current memory usage\"\"\"\n        process = psutil.Process()\n        memory_info = process.memory_info()\n        return memory_info.rss / 1024**2  # MB\n    \n    def benchmark_pytorch(self, model, inputs, n_iterations=100):\n        \"\"\"Benchmark PyTorch model\"\"\"\n        print(f\"\\nBenchmarking PyTorch model ({self.model_name})...\")\n        \n        model.eval()\n        if torch.cuda.is_available():\n            model.cuda()\n            inputs = {k: v.cuda() for k, v in inputs.items()}\n        \n        # Warmup\n        with torch.no_grad():\n            for _ in range(10):\n                _ = model(**inputs)\n        \n        # Benchmark inference time\n        start_time = time.time()\n        with torch.no_grad():\n            for _ in range(n_iterations):\n                _ = model(**inputs)\n        \n        torch.cuda.synchronize() if torch.cuda.is_available() else None\n        end_time = time.time()\n        \n        avg_latency = (end_time - start_time) * 1000 / n_iterations  # ms\n        \n        # Measure memory\n        memory_before = self.measure_memory()\n        with torch.no_grad():\n            _ = model(**inputs)\n        memory_after = self.measure_memory()\n        memory_usage = memory_after - memory_before\n        \n        # Get model size\n        param_size = 0\n        for param in model.parameters():\n            param_size += param.nelement() * param.element_size()\n        model_size_mb = param_size / 1024**2\n        \n        self.results['pytorch'] = {\n            'avg_latency_ms': avg_latency,\n            'memory_usage_mb': memory_usage,\n            'model_size_mb': model_size_mb,\n            'throughput_qps': 1000 / avg_latency  # Queries per second\n        }\n        \n        print(f\"  Average latency: {avg_latency:.2f} ms\")\n        print(f\"  Model size: {model_size_mb:.2f} MB\")\n        print(f\"  Memory usage: {memory_usage:.2f} MB\")\n        print(f\"  Throughput: {1000/avg_latency:.2f} QPS\")\n        \n        return self.results['pytorch']\n    \n    def benchmark_onnx(self, session, inputs, n_iterations=100):\n        \"\"\"Benchmark ONNX model\"\"\"\n        print(f\"\\nBenchmarking ONNX model...\")\n        \n        # Prepare ONNX inputs\n        ort_inputs = {\n            'input_ids': inputs['input_ids'].cpu().numpy(),\n            'attention_mask': inputs['attention_mask'].cpu().numpy()\n        }\n        \n        # Warmup\n        for _ in range(10):\n            _ = session.run(None, ort_inputs)\n        \n        # Benchmark\n        start_time = time.time()\n        for _ in range(n_iterations):\n            _ = session.run(None, ort_inputs)\n        end_time = time.time()\n        \n        avg_latency = (end_time - start_time) * 1000 / n_iterations\n        \n        # Measure memory\n        memory_before = self.measure_memory()\n        _ = session.run(None, ort_inputs)\n        memory_after = self.measure_memory()\n        memory_usage = memory_after - memory_before\n        \n        # Get ONNX model size\n        model_size_bytes = os.path.getsize(session._model_path)\n        model_size_mb = model_size_bytes / 1024**2\n        \n        self.results['onnx'] = {\n            'avg_latency_ms': avg_latency,\n            'memory_usage_mb': memory_usage,\n            'model_size_mb': model_size_mb,\n            'throughput_qps': 1000 / avg_latency\n        }\n        \n        print(f\"  Average latency: {avg_latency:.2f} ms\")\n        print(f\"  Model size: {model_size_mb:.2f} MB\")\n        print(f\"  Memory usage: {memory_usage:.2f} MB\")\n        print(f\"  Throughput: {1000/avg_latency:.2f} QPS\")\n        \n        return self.results['onnx']\n    \n    def benchmark_quantized(self, session, inputs, n_iterations=100):\n        \"\"\"Benchmark quantized ONNX model\"\"\"\n        print(f\"\\nBenchmarking Quantized ONNX model...\")\n        \n        ort_inputs = {\n            'input_ids': inputs['input_ids'].cpu().numpy(),\n            'attention_mask': inputs['attention_mask'].cpu().numpy()\n        }\n        \n        # Warmup\n        for _ in range(10):\n            _ = session.run(None, ort_inputs)\n        \n        # Benchmark\n        start_time = time.time()\n        for _ in range(n_iterations):\n            _ = session.run(None, ort_inputs)\n        end_time = time.time()\n        \n        avg_latency = (end_time - start_time) * 1000 / n_iterations\n        \n        # Memory\n        memory_before = self.measure_memory()\n        _ = session.run(None, ort_inputs)\n        memory_after = self.measure_memory()\n        memory_usage = memory_after - memory_before\n        \n        # Model size\n        model_size_bytes = os.path.getsize(session._model_path)\n        model_size_mb = model_size_bytes / 1024**2\n        \n        self.results['quantized'] = {\n            'avg_latency_ms': avg_latency,\n            'memory_usage_mb': memory_usage,\n            'model_size_mb': model_size_mb,\n            'throughput_qps': 1000 / avg_latency\n        }\n        \n        print(f\"  Average latency: {avg_latency:.2f} ms\")\n        print(f\"  Model size: {model_size_mb:.2f} MB\")\n        print(f\"  Memory usage: {memory_usage:.2f} MB\")\n        print(f\"  Throughput: {1000/avg_latency:.2f} QPS\")\n        \n        return self.results['quantized']\n\n# Cell 8: Run PyTorch benchmark\nbenchmark = ModelBenchmark(\"Distilled XLM-RoBERTa\")\npytorch_results = benchmark.benchmark_pytorch(model, sample_inputs, config.N_ITERATIONS)\n\n# Cell 9: Run ONNX benchmark\nonnx_results = benchmark.benchmark_onnx(onnx_session, sample_inputs, config.N_ITERATIONS)\n\n# Cell 10: Apply Dynamic Quantization to ONNX model\ndef quantize_onnx_model(onnx_path, output_path):\n    \"\"\"Apply dynamic quantization to ONNX model\"\"\"\n    print(f\"\\nApplying dynamic quantization to ONNX model...\")\n    \n    from onnxruntime.quantization import quantize_dynamic, QuantType\n    \n    quantized_model = quantize_dynamic(\n        onnx_path,\n        output_path,\n        weight_type=QuantType.QInt8,\n        optimize_model=True\n    )\n    \n    print(f\"Quantized model saved to {output_path}\")\n    \n    # Check size reduction\n    original_size = os.path.getsize(onnx_path)\n    quantized_size = os.path.getsize(output_path)\n    reduction = (1 - quantized_size / original_size) * 100\n    \n    print(f\"  Original size: {original_size / 1024**2:.2f} MB\")\n    print(f\"  Quantized size: {quantized_size / 1024**2:.2f} MB\")\n    print(f\"  Size reduction: {reduction:.1f}%\")\n    \n    return output_path\n\nquantized_path = os.path.join(config.OUTPUT_DIR, \"quantized\", \"model_quantized.onnx\")\nquantized_path = quantize_onnx_model(onnx_path, quantized_path)\n\n# Cell 11: Create quantized session and benchmark\nquantized_session = create_onnx_session(quantized_path)\nquantized_results = benchmark.benchmark_quantized(quantized_session, sample_inputs, config.N_ITERATIONS)\n\n# Cell 12: Compare results\ndef compare_results(benchmark_results):\n    print(\"\\n\" + \"=\"*60)\n    print(\"MODEL COMPARISON SUMMARY\")\n    print(\"=\"*60)\n    \n    models = ['pytorch', 'onnx', 'quantized']\n    headers = [\"Model\", \"Size (MB)\", \"Latency (ms)\", \"Throughput (QPS)\", \"Speedup\"]\n    \n    print(f\"{headers[0]:<15} {headers[1]:<12} {headers[2]:<15} {headers[3]:<15} {headers[4]:<10}\")\n    print(\"-\"*60)\n    \n    pytorch_latency = benchmark_results['pytorch']['avg_latency_ms']\n    \n    for model in models:\n        if model in benchmark_results:\n            res = benchmark_results[model]\n            speedup = pytorch_latency / res['avg_latency_ms']\n            \n            print(f\"{model:<15} {res['model_size_mb']:<12.2f} \"\n                  f\"{res['avg_latency_ms']:<15.2f} \"\n                  f\"{res['throughput_qps']:<15.2f} \"\n                  f\"{speedup:<10.2f}x\")\n\ncompare_results(benchmark.results)\n\n# Cell 13: Accuracy validation\ndef validate_accuracy(original_model, onnx_session, quantized_session, tokenizer, test_samples=50):\n    \"\"\"Validate that quantized model maintains accuracy\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"ACCURACY VALIDATION\")\n    print(\"=\"*60)\n    \n    from datasets import load_dataset\n    import random\n    \n    # Load test data\n    dataset = load_dataset(\"wikiann\", \"en\")\n    test_data = dataset['test'].select(range(test_samples))\n    \n    correct_original = 0\n    correct_onnx = 0\n    correct_quantized = 0\n    total_tokens = 0\n    \n    for i in range(min(test_samples, len(test_data))):\n        sample = test_data[i]\n        tokens = sample['tokens']\n        true_labels = sample['ner_tags']\n        \n        # Tokenize\n        inputs = tokenizer(\n            tokens,\n            is_split_into_words=True,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=config.MAX_LENGTH,\n            padding=\"max_length\"\n        )\n        \n        # Original model prediction\n        with torch.no_grad():\n            outputs = original_model(**inputs)\n        original_preds = torch.argmax(outputs.logits, dim=-1)[0].cpu().numpy()\n        \n        # ONNX prediction\n        ort_inputs = {\n            'input_ids': inputs['input_ids'].cpu().numpy(),\n            'attention_mask': inputs['attention_mask'].cpu().numpy()\n        }\n        onnx_outputs = onnx_session.run(None, ort_inputs)\n        onnx_preds = np.argmax(onnx_outputs[0], axis=-1)[0]\n        \n        # Quantized prediction\n        quantized_outputs = quantized_session.run(None, ort_inputs)\n        quantized_preds = np.argmax(quantized_outputs[0], axis=-1)[0]\n        \n        # Align predictions with words\n        word_ids = inputs.word_ids(batch_index=0)\n        previous_word_idx = None\n        \n        original_aligned = []\n        onnx_aligned = []\n        quantized_aligned = []\n        \n        for idx, word_idx in enumerate(word_ids):\n            if word_idx is not None and word_idx != previous_word_idx:\n                if word_idx < len(true_labels):\n                    original_aligned.append(original_preds[idx])\n                    onnx_aligned.append(onnx_preds[idx])\n                    quantized_aligned.append(quantized_preds[idx])\n            previous_word_idx = word_idx\n        \n        # Compare\n        if len(original_aligned) == len(true_labels):\n            correct_original += sum(1 for o, t in zip(original_aligned, true_labels) if o == t)\n            correct_onnx += sum(1 for o, t in zip(onnx_aligned, true_labels) if o == t)\n            correct_quantized += sum(1 for q, t in zip(quantized_aligned, true_labels) if q == t)\n            total_tokens += len(true_labels)\n    \n    # Calculate accuracies\n    accuracy_original = correct_original / total_tokens if total_tokens > 0 else 0\n    accuracy_onnx = correct_onnx / total_tokens if total_tokens > 0 else 0\n    accuracy_quantized = correct_quantized / total_tokens if total_tokens > 0 else 0\n    \n    print(f\"Original Model Accuracy: {accuracy_original:.4f}\")\n    print(f\"ONNX Model Accuracy:     {accuracy_onnx:.4f}\")\n    print(f\"Quantized Model Accuracy: {accuracy_quantized:.4f}\")\n    print(f\"\\nAccuracy drop (vs Original):\")\n    print(f\"  ONNX:     {(accuracy_original - accuracy_onnx):.4f}\")\n    print(f\"  Quantized: {(accuracy_original - accuracy_quantized):.4f}\")\n    \n    return {\n        'original_accuracy': accuracy_original,\n        'onnx_accuracy': accuracy_onnx,\n        'quantized_accuracy': accuracy_quantized\n    }\n\naccuracy_results = validate_accuracy(model, onnx_session, quantized_session, tokenizer)\n\n# Cell 14: Save optimization results\ndef save_optimization_results(benchmark_results, accuracy_results, config):\n    results = {\n        'benchmark': benchmark_results,\n        'accuracy': accuracy_results,\n        'config': {\n            'model_path': config.MODEL_PATH,\n            'max_length': config.MAX_LENGTH,\n            'iterations': config.N_ITERATIONS\n        },\n        'summary': {\n            'size_reduction': {\n                'pytorch_to_onnx': (\n                    benchmark_results['pytorch']['model_size_mb'] - \n                    benchmark_results['onnx']['model_size_mb']\n                ),\n                'pytorch_to_quantized': (\n                    benchmark_results['pytorch']['model_size_mb'] - \n                    benchmark_results['quantized']['model_size_mb']\n                ),\n                'percentage_reduction': (\n                    (1 - benchmark_results['quantized']['model_size_mb'] / \n                     benchmark_results['pytorch']['model_size_mb']) * 100\n                )\n            },\n            'speedup': {\n                'pytorch_to_onnx': (\n                    benchmark_results['pytorch']['avg_latency_ms'] / \n                    benchmark_results['onnx']['avg_latency_ms']\n                ),\n                'pytorch_to_quantized': (\n                    benchmark_results['pytorch']['avg_latency_ms'] / \n                    benchmark_results['quantized']['avg_latency_ms']\n                )\n            }\n        }\n    }\n    \n    output_path = os.path.join(config.OUTPUT_DIR, \"optimization_results.json\")\n    with open(output_path, 'w') as f:\n        json.dump(results, f, indent=2)\n    \n    print(f\"\\nOptimization results saved to {output_path}\")\n    \n    # Print summary\n    print(\"\\n\" + \"=\"*60)\n    print(\"OPTIMIZATION SUMMARY\")\n    print(\"=\"*60)\n    print(f\"Model size reduction: {results['summary']['size_reduction']['percentage_reduction']:.1f}%\")\n    print(f\"Quantized model speedup: {results['summary']['speedup']['pytorch_to_quantized']:.2f}x\")\n    print(f\"Accuracy drop: {(accuracy_results['original_accuracy'] - accuracy_results['quantized_accuracy']):.4f}\")\n    \n    return results\n\nfinal_results = save_optimization_results(benchmark.results, accuracy_results, config)\n\n# Cell 15: Create visualization\ndef create_optimization_visualization(results, config):\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    \n    # Prepare data\n    models = ['PyTorch', 'ONNX', 'Quantized ONNX']\n    latencies = [\n        results['benchmark']['pytorch']['avg_latency_ms'],\n        results['benchmark']['onnx']['avg_latency_ms'],\n        results['benchmark']['quantized']['avg_latency_ms']\n    ]\n    sizes = [\n        results['benchmark']['pytorch']['model_size_mb'],\n        results['benchmark']['onnx']['model_size_mb'],\n        results['benchmark']['quantized']['model_size_mb']\n    ]\n    throughput = [\n        results['benchmark']['pytorch']['throughput_qps'],\n        results['benchmark']['onnx']['throughput_qps'],\n        results['benchmark']['quantized']['throughput_qps']\n    ]\n    \n    # Create subplots\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n    # Plot 1: Latency comparison\n    axes[0, 0].bar(models, latencies, color=['blue', 'orange', 'green'])\n    axes[0, 0].set_title('Inference Latency Comparison')\n    axes[0, 0].set_ylabel('Latency (ms)')\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    # Add values on bars\n    for i, v in enumerate(latencies):\n        axes[0, 0].text(i, v, f'{v:.1f}', ha='center', va='bottom')\n    \n    # Plot 2: Model size comparison\n    axes[0, 1].bar(models, sizes, color=['blue', 'orange', 'green'])\n    axes[0, 1].set_title('Model Size Comparison')\n    axes[0, 1].set_ylabel('Size (MB)')\n    axes[0, 1].grid(True, alpha=0.3)\n    \n    for i, v in enumerate(sizes):\n        axes[0, 1].text(i, v, f'{v:.1f}', ha='center', va='bottom')\n    \n    # Plot 3: Throughput comparison\n    axes[1, 0].bar(models, throughput, color=['blue', 'orange', 'green'])\n    axes[1, 0].set_title('Throughput Comparison')\n    axes[1, 0].set_ylabel('Queries per Second')\n    axes[1, 0].grid(True, alpha=0.3)\n    \n    for i, v in enumerate(throughput):\n        axes[1, 0].text(i, v, f'{v:.1f}', ha='center', va='bottom')\n    \n    # Plot 4: Speedup comparison\n    speedup = [1, latencies[0]/latencies[1], latencies[0]/latencies[2]]\n    axes[1, 1].bar(models, speedup, color=['blue', 'orange', 'green'])\n    axes[1, 1].set_title('Speedup vs PyTorch')\n    axes[1, 1].set_ylabel('Speedup (x)')\n    axes[1, 1].grid(True, alpha=0.3)\n    \n    for i, v in enumerate(speedup):\n        axes[1, 1].text(i, v, f'{v:.2f}x', ha='center', va='bottom')\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(config.OUTPUT_DIR, \"optimization_comparison.png\"), dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    # Create summary table\n    summary_df = pd.DataFrame({\n        'Model': models,\n        'Size (MB)': sizes,\n        'Latency (ms)': latencies,\n        'Throughput (QPS)': throughput,\n        'Speedup': speedup,\n        'Accuracy': [\n            results['accuracy']['original_accuracy'],\n            results['accuracy']['onnx_accuracy'],\n            results['accuracy']['quantized_accuracy']\n        ]\n    })\n    \n    print(\"\\nOptimization Summary Table:\")\n    print(\"-\"*80)\n    print(summary_df.to_string(index=False))\n    print(\"-\"*80)\n    \n    # Save table\n    summary_df.to_csv(os.path.join(config.OUTPUT_DIR, \"optimization_summary.csv\"), index=False)\n    \n    return fig, summary_df\n\nfig, summary_df = create_optimization_visualization(final_results, config)\n\n# Cell 16: Test inference with quantized model\ndef test_quantized_inference(session, tokenizer, text):\n    \"\"\"Test inference with quantized model\"\"\"\n    print(f\"\\nTesting quantized model inference:\")\n    print(f\"Text: {text}\")\n    \n    tokens = text.split()\n    \n    # Prepare inputs\n    inputs = tokenizer(\n        tokens,\n        is_split_into_words=True,\n        return_tensors=\"np\",  # Use numpy for ONNX\n        truncation=True,\n        max_length=config.MAX_LENGTH,\n        padding=\"max_length\"\n    )\n    \n    ort_inputs = {\n        'input_ids': inputs['input_ids'],\n        'attention_mask': inputs['attention_mask']\n    }\n    \n    # Run inference\n    start_time = time.time()\n    outputs = session.run(None, ort_inputs)\n    inference_time = (time.time() - start_time) * 1000  # ms\n    \n    predictions = np.argmax(outputs[0], axis=-1)[0]\n    \n    # Extract entities\n    word_ids = inputs.word_ids(batch_index=0)\n    previous_word_idx = None\n    predictions_aligned = []\n    \n    for idx, word_idx in enumerate(word_ids):\n        if word_idx is not None and word_idx != previous_word_idx:\n            predictions_aligned.append(predictions[idx])\n        previous_word_idx = word_idx\n    \n    # Format output\n    entities = []\n    current_entity = None\n    current_start = None\n    current_label = None\n    \n    for i, (token, pred_idx) in enumerate(zip(tokens, predictions_aligned)):\n        label = config.LABEL_NAMES[pred_idx]\n        \n        if label.startswith(\"B-\"):\n            if current_entity:\n                entities.append({\n                    \"entity\": \" \".join(tokens[current_start:i]),\n                    \"label\": current_label,\n                    \"start\": current_start,\n                    \"end\": i,\n                    \"score\": 1.0  # Placeholder\n                })\n            current_label = label[2:]\n            current_start = i\n            current_entity = [token]\n        elif label.startswith(\"I-\") and current_label == label[2:]:\n            current_entity.append(token)\n        elif current_entity:\n            entities.append({\n                \"entity\": \" \".join(current_entity),\n                \"label\": current_label,\n                \"start\": current_start,\n                \"end\": i,\n                \"score\": 1.0\n            })\n            current_entity = None\n            current_label = None\n            current_start = None\n    \n    if current_entity:\n        entities.append({\n            \"entity\": \" \".join(current_entity),\n            \"label\": current_label,\n            \"start\": current_start,\n            \"end\": len(tokens),\n            \"score\": 1.0\n        })\n    \n    print(f\"Inference time: {inference_time:.2f} ms\")\n    print(f\"Entities found: {entities}\")\n    \n    return entities, inference_time\n\n# Test with multiple languages\ntest_texts = [\n    (\"English\", \"Apple was founded by Steve Jobs in Cupertino.\"),\n    (\"German\", \"Berlin ist die Hauptstadt von Deutschland.\"),\n    (\"French\", \"Paris est la capitale de la France.\"),\n    (\"Spanish\", \"Google es una empresa de tecnología con sede en Mountain View.\")\n]\n\nfor lang, text in test_texts:\n    print(f\"\\n[{lang}]\")\n    entities, inference_time = test_quantized_inference(quantized_session, tokenizer, text)\n\n# Cell 17: Create deployment artifacts\ndef create_deployment_artifacts(config, tokenizer, quantized_session):\n    \"\"\"Create artifacts needed for deployment\"\"\"\n    artifacts_dir = os.path.join(config.OUTPUT_DIR, \"deployment\")\n    os.makedirs(artifacts_dir, exist_ok=True)\n    \n    # 1. Save tokenizer\n    tokenizer.save_pretrained(artifacts_dir)\n    \n    # 2. Save model config\n    model_config = {\n        \"model_type\": \"onnx_quantized\",\n        \"max_length\": config.MAX_LENGTH,\n        \"label_names\": config.LABEL_NAMES,\n        \"num_labels\": config.NUM_LABELS,\n        \"optimization_results\": {\n            \"latency_ms\": final_results['benchmark']['quantized']['avg_latency_ms'],\n            \"size_mb\": final_results['benchmark']['quantized']['model_size_mb'],\n            \"accuracy\": final_results['accuracy']['quantized_accuracy']\n        }\n    }\n    \n    with open(os.path.join(artifacts_dir, \"config.json\"), \"w\") as f:\n        json.dump(model_config, f, indent=2)\n    \n    # 3. Copy quantized model\n    import shutil\n    shutil.copy2(\n        quantized_path,\n        os.path.join(artifacts_dir, \"model.onnx\")\n    )\n    \n    # 4. Create requirements file\n    requirements = [\n        \"onnxruntime>=1.15.0\",\n        \"transformers>=4.30.0\",\n        \"tokenizers>=0.13.0\",\n        \"numpy>=1.24.0\"\n    ]\n    \n    with open(os.path.join(artifacts_dir, \"requirements.txt\"), \"w\") as f:\n        f.write(\"\\n\".join(requirements))\n    \n    # 5. Create inference script template\n    inference_template = '''import onnxruntime as ort\nimport numpy as np\nfrom transformers import AutoTokenizer\nimport json\n\nclass MultilingualNER:\n    def __init__(self, model_path, tokenizer_path):\n        self.session = ort.InferenceSession(model_path)\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n        \n        with open(f\"{tokenizer_path}/config.json\", \"r\") as f:\n            self.config = json.load(f)\n    \n    def predict(self, text, language=\"en\"):\n        # Implementation here\n        pass\n\nif __name__ == \"__main__\":\n    # Example usage\n    model = MultilingualNER(\"model.onnx\", \".\")\n    result = model.predict(\"Apple is in Cupertino.\")\n    print(result)\n'''\n    \n    with open(os.path.join(artifacts_dir, \"inference_example.py\"), \"w\") as f:\n        f.write(inference_template)\n    \n    print(f\"\\nDeployment artifacts created in {artifacts_dir}\")\n    print(\"Contents:\")\n    for item in os.listdir(artifacts_dir):\n        print(f\"  - {item}\")\n\ncreate_deployment_artifacts(config, tokenizer, quantized_session)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"OPTIMIZATION COMPLETED SUCCESSFULLY!\")\nprint(\"=\"*60)\nprint(f\"Final quantized model: {final_results['benchmark']['quantized']['model_size_mb']:.1f} MB\")\nprint(f\"Inference latency: {final_results['benchmark']['quantized']['avg_latency_ms']:.1f} ms\")\nprint(f\"Throughput: {final_results['benchmark']['quantized']['throughput_qps']:.1f} QPS\")\nprint(f\"Accuracy: {final_results['accuracy']['quantized_accuracy']:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}